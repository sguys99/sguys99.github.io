<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!--custom.css-->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />

    <!-Font Awesome-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <!--웹폰트 추가-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300&display=swap" />

    <!--syntax.css 추가-->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />


    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Data science, Machine learning, and Automatic control" />
    <link rel="shortcut icon" href="https://sguys99.github.io//assets/images/logo.png" type="image/png" />
    <link rel="canonical" href="https://sguys99.github.io//search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Happy Plant" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="Data science, Machine learning, and Automatic control" />
    <meta property="og:url" content="https://sguys99.github.io//search" />
    <meta property="og:image" content="https://sguys99.github.io//assets/images/main-cover.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/dbrhkdaud" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="Data science, Machine learning, and Automatic control" />
    <meta name="twitter:url" content="https://sguys99.github.io//" />
    <meta name="twitter:image" content="https://sguys99.github.io//assets/images/main-cover.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Happy Plant" />
    <meta name="twitter:site" content="@sguys99" />
    <meta name="twitter:creator" content="@sguys99" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Happy Plant",
        "logo": "https://sguys99.github.io//"
    },
    "url": "https://sguys99.github.io//search",
    "image": {
        "@type": "ImageObject",
        "url": "https://sguys99.github.io//assets/images/main-cover.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://sguys99.github.io//search"
    },
    "description": "Data science, Machine learning, and Automatic control"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://sguys99.github.io//">Happy Plant</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-data-science" role="menuitem"><a href="/tag/data-science/">Data Science</a></li>
    <li class="nav-algorithmic-trading" role="menuitem"><a href="/tag/algorithmic-trading/">Trading</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/it-tips/">It Tips</a></li>
    <li class="nav-diary" role="menuitem"><a href="/tag/diary/">Diary</a></li>
    <!-- 주석
    <li class="nav-goto-turtles3040" role="menuitem"><a href="https://github.com/turtles3040">turtles3040</a></li>
    -->
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Posts by Tag</a>
    </li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
                <a class="social-link social-link-fb" href="https://facebook.com/dbrhkdaud" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>
</a>
            
            
            <a class="social-link social-link-tw" href="https://www.github.com/sguys99" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-github" viewBox="0 0 16 16">
  <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
</svg></a>
            
            
            <a class="social-link social-link-tw" href="https://www.linkedin.com/in/kmyu99" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1"  width="24" height="24" viewBox="0 0 24 24"><path fill="#FFFFFF" d="M21,21H17V14.25C17,13.19 15.81,12.31 14.75,12.31C13.69,12.31 13,13.19 13,14.25V21H9V9H13V11C13.66,9.93 15.36,9.24 16.5,9.24C19,9.24 21,11.28 21,13.75V21M7,21H3V9H7V21M5,3A2,2 0 0,1 7,5A2,2 0 0,1 5,7A2,2 0 0,1 3,5A2,2 0 0,1 5,3Z" /></svg></a>
            
            
                <a class="social-link social-link-tw" href="https://twitter.com/sguys99" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "foody7": {
        "title": "(서울 송파 맛집) 셰프의 부엌",
            "author": "km.yu99",
            "category": "",
            "content": "송파구 방이시장 인근 파스타 맛집 셰프의 부엌  주 소 :  서울 송파구 가락로 259  영업시간 : 17:00 ~ 00:00 (매월 2, 4주차 일요일 휴무)  전화번호 : 010-4762-3037  대표메뉴 : 알리오올리오(14,000원), 날치알크림파스타(18,000원), 카수엘라감바스(20,000원)신혼시절 집근처에 자주가던 식당이 있었다. 파스타도 팔고 가볍게 맥주나 와인 한잔하며 요기를 하던 곳. 출출할 때 아내와 처제를 불러내서 제법 근사한 안주에 폼내며 한잔하고 들어 갔던 곳. 그런데 언젠가 가게가 있던 건물은 전체가 재개발이 들어가고 그 식당도 사라졌다. 자연스럽게 내 기억속에서 멀어져갔다.얼마 전 퇴근길에 우연히 다른 골목에서 같은 상호의 가게를 발견했다. 외관과 인테리어는 많이 다르지만 다행히 같은 주인이 운영하는 가게였다. 당시 기억엔 갈때마다 손님이 우리 밖에 없었는데 큰길가로 옮긴 덕인지 매일 손님은 만석이다. 음식의 질과 서비스를 생각하면 당연하겠지만… 오늘은 나의 추억이 깃든 이 장소에 대해 포스팅해보려고 한다.가게 내부는 4인 테이블 5개 정도로 크지 않다. 주말에 손님이 붐빌 것 같아 미리 예약을 하고 방문했다. 역시 모든 테이블이 예약 표시되어 있다. 이전의 가게는 스페인 느낌이 물씬 풍기는 엔틱풍의 인테리어가 인상적이었다. 새로 옮긴 가게는 더 모던해진 느낌이다.이곳의 식사도 훌륭하지만 한잔 하면서 즐기는 안주도 일품이다. 이날은 일정이 있어 식사만 하기로 하고 안주는 다음을 기약하기로 한다. 서빙되는 음식들의 퀄리티를 생각하면 가격도 나쁘지 않은 편이다.과거에 비해 정갈하고 깔끔해진 테이블 세팅. 뭐든 좋다.음식을 기다리며 창밖을 내다본다. 간만에 즐기는 여유. 자주 걷던 동네 인근인데도, 지나가는 차와 사람들을 관찰하는 재미가 있다.에피타이저로 치즈와 크래커가 나왔다. 짭조름하면서 달달한 치즈가 담백한 크래커와 절묘하게 어우러진다. 음식을 기다리기 힘들게 더 안달나게 한다고나 할까.알리오올리오 파스타가 먼저 나왔다. 이 곳에 오면 항상 주문하는, 강력 추천하는 메뉴다. 1,4000원이라는 가격도 마음에 든다. 보는 재미도 맛보는 재미도 있는 메뉴.올리브오일의 고소함과 고추의 칼칼함이 더해져서 개성이 강한 맛을 낸다. 이 곳에서만 경험 할 수 있는 맛이다. 알리오 올리오가 자칫 심심해 질수도 있는데 튀김가루와 약간의 베이컨, 야채 등이 더해져서 씹는 재미도 있다. 여전히 실망시키지 않는 맛이다.이어 등장한 크림 리조또. 어디를 가든 기본 이상을 하는 메뉴. 취향에 따라 알리오 올리오가 맵다고 느낄 수도 있는데, 함께 시키면 궁합이 맞을 것 같다. 가격도 1,5000원으로 요즘 물가에 비해 저렴한 편이다.세명이서 왔을 때, 세개를 시키자니 양이 부담스럽고, 두개를 주문하자니 양이 적을 것 같다면, 제주 돈까스 샐러드를 주문해보자. 두툼한 돈까스와 싱싱한 샐러드 그리고 소스가 어우러진 건강한 맛이다.주말 저녁, 소중한 친구, 가족들이 삼삼오오 모여 멋들어진 음식과 함께 소중한 시간을 나눈다. 5년 만에 재방문한 추억의 장소. 그 누군가에게도 이 가게가 기억에 남는 공간이 되길 바라며 계속 이 맛을 이어가 주길 바란다.총 평  음식맛 : ★★★★☆  가성비 : ★★★★☆  서비스 : ★★★★☆  접근성 : ★★★★☆",
        "url": "/foody7"
    }
    ,
    
    "story05": {
        "title": "나의 이직 이야기 - 4",
            "author": "km.yu99",
            "category": "",
            "content": "- 임소장3 -임소장이 입사한지 일주일이 지났다.본사 지원부서는 끝내 소장 개인 집무실을 허락하지 않았다.소장과 지원부서 담당자는 나를 중간 매개체로 며칠 째 핑퐁을 치고 있다. 서로 맞대면 해서 싫은 소리 직접하기 싫고, 그 누구도 중재하기 싫은 거다. 기분이 나쁘진 않다. 회사 생활에 익숙해지면서 감정이 무뎌질대로 무뎌졌다.지원부서 담당자는 규정상 개인 집무실은 최고 직급에게만 제공된다는 답변만 반복했다.☞ 최고직급은 1직급. 소장은 계약직이며 2 직급.소장은 일주일 동안 본사를 설득하라고 고집했다.오늘에서야 회의 테이블과 칸막이를 설치하는 것으로 협의를 봤다.소장이 입사를 늦추는 바람에 사무가구 시공업체는 사무실을 알아서 공사하고 이미 떠났다.할 수 없이 나를 포함한 C팀 사람들이 지하창고에서 파티션을 짊어지고와 망치를 두들겨 가며 공사를 했다.한번쯤 스윽 내다보며 고맙다는 제스처를 취할법도 한데,소장은 작업시간 내내 자리에 앉아서 PC 모니터만 무심하게 쳐다본다.초반부터 한국식 인사치레는 전혀 없구나…. 아메리칸 스타일인가?일과를 마치고 시간이 되는 사람들 몇을 모아 연탄구이집에서 늦은 식사를 한다.오박사에게는 물어보지도 않았는데 저 뒤에서 우리 무리를 따라온다.연탄구이집까지 따라와서 우리 테이블에 동석한다.웃으면서 재잘거리다가, 오박사가 오니 다들 하던말을 멈춘다.눈치만 보다가 취기가 오르자 다들 한마디씩 하기 시작했다.‘소장님 좀 이상하지 않아요? 불필요한 요구사항도 이것 저것 많고 신혼여행 간다고 회사도 안나오고요.기존 상사들보다 더 권위적인 것 같아요.’오박사가 기다렸다는 듯이 말을 쏟아낸다.‘내가 보기엔 능력도 없고! 생각하는 것도 딱 어린애야 어린애!내눈에는 양복입은 어린애로 밖에 안보여!’오박사의 설교는 30분간 계속됐다.어르신이 하는 말씀이라 음식을 먹지도 못하고, 타는 고기를 곁눈으로 봐야만 했다.겉으로는 베시시 미소를 지으며, ‘허허’, ‘네네’하고 오박사 이야기에 동조하는 시늉을 했다.오박사 말을 전적으로 수긍하기는 어려웠다.그러기에는 오박사와 소장의 성격이나 행동에 공통 분모가 너무 많았다.한시간 가까이 혼자 연설하시는 오박사님.‘저, 박사님? 내일 연구소 개소식이라 일찍 일어나야 할 것 같습니다.’‘그래? 아무튼 문제가 많아. 다 먹었지? 이제 일어 나자구.’오늘 저녁식사 망했다.  직원들과 삼삼오오 모여 뒷담화와 고충을 늘어놓던 연탄구이집. 가격은 좀 되지만 생근고기 세트와 된잡국밥 코스는 회포를 풀기에 충분하다.",
        "url": "/story05"
    }
    ,
    
    "story04": {
        "title": "나의 이직 이야기 - 3",
            "author": "km.yu99",
            "category": "",
            "content": "- 임소장2 -‘여러분, 데이터 사이언스에 대해서 뭐좀 아세요? 아니 랩실에서 한거 말고….’‘내말은, 실제 적용해서 시스템 구현까지 해보셨어요?’경력직으로 입사한 오박사가 우리 5명을 회의실로 소집했다.하루에 한번 꼴로 불러내서 훈계를 한다. 소장은 아직까지도 출근하지 않았다.  오박사 : 55세. 소장(50세) 보다 나이 많음훈계가 또 길어진다. 지겹다. 듣는척하고 오박사의 얼굴 생김새만 살펴본다.왁스를 발라 넘긴 백발이 형광등에 비쳐 유난히 반짝인다.입사 연차로는 내가 7년 선배다.오박사가 가장 연장자여서 우리에게 이런저런 지시를 많이 내리고 회의 소집도 자주했다.‘나는 민간에서 10년 넘게 일했어요. 여기는 전문가도 없고 인프라도 없어서 걱정입니다.’그의 어투에서 자만심과 포함한 사내 공모인력에 대한 냉소적 태도가 느껴진다.아직도 출근안한 임소장에 대한 이야기도 꺼낸다.‘나는 좋은 학교 나왔다고 상사로 인정 안해요. 나보다 실력이 월등해야 소장으로 인정할거에요.’‘내가 미국 학위만 있었어도 소장감이지, 안그래 박박사?’‘우리들끼리 잘 뭉쳐야해요. 그러니 여러분이 나 힘좀 실어주세요.’아무리 자신감이 넘친다지만 머릿속에서나 굴릴 버릴 생각들을 필터링 없이 내뱉다니…‘아, 그리고!’‘소장 없다고 마냥 쉴수 없으니 팀을 나눠서 프로젝트나 하고 있자구요.’‘세팀 정도로 나누면 좋을 것 같은데…. 나와 박박사가 여기서 가장 능력이 있어 보이니 A팀장, B팀장을 맡을게요.’‘C팀장은 누가 할래요?’내가 손들었다.‘제가 할게요. 여기서 나이가 어린축에 속하긴한데 지원자가 없으면 제가 할게요.’자신있어서 손든건 아니다. 박박사랑 같은팀으로 엮이면 피곤할 것 같았다.나도 이 회사 짬만 12년이다. 누구에게 붙어야 살아남을지 본능적으로 안다.저사람은 아니다.나와 같이 차출된 사내공모 차장 두명이 나와 같은 C팀에 가겠다고 손들었다.그들도 안다. 같이 있으면 피곤해 질 것을…‘그럼 남은 차장은 나랑 한팀하면 되겠네. 자 그럼 일 합시다. 능력 키워야지!’…며칠 뒤 소장이 출근했다. 연구소 개소 후 2주만이다.다들 사무실 앞에 사열해서 자본주의 미소를 띄며 소장과 악수를 청한다.오박사만은 예외다.왼손은 바지 주머니에 넣고 악수랍시고 손바닥만 스치고 자리에 가서 앉는다.오박사도 그렇지만 소장의 첫인상도 예사롭지 않다. 경직되어 있는건지, 기분이 안좋은 건지…모르겠다.직원들 인사에 대한 화답으로 약간의 미소라도 지을 법한데 시종일관 무표정이다.아니 약간 화난 얼굴에 가깝다.말수도 적고, 목소리도 작다.귀를 쫑긋하지 않으면 잘 들리지 않는다.소장은 인사를 나눈 후 자리로 가서 pc를 켰다.일을 하다가 우연히 나와 눈이 마주쳤다.나를 바라보며 뭐라고 웅얼거린다.‘네?’한마디도 안들린다. 또 웅얼웅얼…자리로 가서 다시 물었다.‘뭐 필요하신 거라도?’‘왜 내 개인 사무실 만들지 않았죠?’‘아, 네. 지원부서에서 다 같이 한 사무실에서 근무하도록 배치를 했습니다.’‘저희가 구x 코리아도 가보고 ‘카xx’ 도 가봤는데요. 요즘 IT 기업들이 다들 좌석 배치를 이렇게…..’어색한 웃음을 지으며 주절거린다.소장은 특유의 무표정으로 웅얼거리며 답했다.‘내일까지 개인 집무실 하나 만들어놔요.’",
        "url": "/story04"
    }
    ,
    
    "ds4": {
        "title": "Timeseries Analysis1 - datetime 라이브러리",
            "author": "km.yu99",
            "category": "",
            "content": "시간정보를 다루는 python 모듈에는 datetime, dateutil 등이 있다. pandas 라이브러리는 시계열 데이터를 다룰 때 내부적으로 이 모듈들을 조합해서 사용한다.본 포스트에서는 python 환경에서 pnadas로 시계열 분석을 위한 기본 라이브러리에 대해서 살펴본다.0. Library importimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport datetimeimport warnings; warnings.filterwarnings('ignore')plt.style.use('ggplot')%matplotlib inline1. datetime library날짜정보 객체 만들기. datetime 라이브러리 안에 datetime 메서드를 사용한다.my_birthday = datetime.datetime(1980, 5, 28)다음과 같이 라이브러리를 import 하자.from datetime import datetimemy_birthday = datetime(1980, 5, 28)my_birthdaydatetime.datetime(1980, 5, 28, 0, 0)데이터 타입을 확인해보자. datetime.datetime 포맷으로 표시된다.type(my_birthday)datetime.datetime현재 날짜와 시간 구하기 1today = datetime.today()todaydatetime.datetime(2022, 1, 9, 14, 33, 52, 922236)현재 날짜와 시간 구하기 2today = datetime.now()todaydatetime.datetime(2022, 1, 9, 14, 33, 53, 834119)print(today)2022-01-09 14:33:53.834119연-월-일 시:분:초 형태로 표현된다.strftime 메서드 : 날짜정보를 문자열로 리턴한다. 메서드 인자로 문자열 포멧을 지정할 수 있다. 포멧에 대한 자세한 내용은 [이곳]을 참고한다.today = today.strftime('%Y-%m-%d')print(today)2022-01-09type(today)str시간 정보를 가져올 수도 있다.today = datetime.now()nowTime = today.strftime('%H:%M:%S')print(nowTime)18:39:05날짜, 시간정보 가져오기.today = datetime.now()todayTime = today.strftime('%Y-%m-%d %H:%M:%S')print(todayTime)2020-07-19 18:39:05해당 일의 요일 정보를 가져올 수도 있다.today = datetime.now()day = today.strftime('%A')print(day)Sundayday = today.strftime('%a')print(day)Sunstrptime 메서드 : 문자열을 datetime 포맷으로 변환한다.my_birthday = '1980-05-28 14:30:00'my_birthday = datetime.strptime(my_birthday, '%Y-%m-%d %H:%M:%S')print(type(my_birthday))print(my_birthday)    &lt;class 'datetime.datetime'&gt;    1980-05-28 14:30:00날짜, 시간 정보 인덱싱하기today = datetime.now()date = today.date()print(date)    2022-01-09nowTime = today.time()print(nowTime)14:43:28.290642날짜, 시간정보 합치기dateTime = datetime.combine(date, nowTime)print(dateTime)2022-01-09 14:43:28.290642년(year), 월(month), 일(day) 정보를 인덱싱하기today = datetime.now()print(today)2022-01-09 14:43:55.073929print(today.year)print(today.month)print(today.day)    2022    1    9날짜 연산from datetime import timedeltatoday = datetime.now().date()print(today)2022-01-09tomorrow = today + timedelta(days=1)print(tomorrow)2022-01-10days 대신에 사용할 수 있는 인자 값 : weeks, hours, minutes, seconds 등dates = [datetime(2022, 1, 9), datetime(2022, 1, 9)+timedelta(days= 1)]dates[datetime.datetime(2022, 1, 9, 0, 0), datetime.datetime(2022, 1, 10, 0, 0)]dates = [datetime(2022, 1, 9), datetime(2022, 1, 9)+timedelta(days= 20)]dates[datetime.datetime(2022, 1, 9, 0, 0), datetime.datetime(2022, 1, 29, 0, 0)]2. numpy의 datetime64numpy에는 시계열 정보를 정교하게 저장하기 위한 datetime64라는 것이 존재한다.date = np.array('2022-01-09', dtype=np.datetime64)datearray('2022-01-09', dtype='datetime64[D]')datetime 라이브러리와 호환도 가능하다.np.datetime64(dateTime.now(), 'ns')numpy.datetime64('2022-01-09T15:09:57.298117000')날짜 연산도 가능하다.date + np.arange(20)    array(['2022-01-09', '2022-01-10', '2022-01-11', '2022-01-12',           '2022-01-13', '2022-01-14', '2022-01-15', '2022-01-16',           '2022-01-17', '2022-01-18', '2022-01-19', '2022-01-20',           '2022-01-21', '2022-01-22', '2022-01-23', '2022-01-24',           '2022-01-25', '2022-01-26', '2022-01-27', '2022-01-28'],          dtype='datetime64[D]')3. pandas built-in 라이브러리pandas에서는 Timestamp라는 객체를 내부적으로 사용한다. 이 것으로 앞에 소개한 datetime, dateutil, 그리고 numpy의 datetime64를 편하게 조합할 수 있다.to_datetime : 문자열을 Timestamp 포맷을 변경한다.today = pd.to_datetime('2022-01-09')today    Timestamp('2022-01-09 00:00:00')날짜 정보 parsing도 가능하다.my_birthday = pd.to_datetime(\"28th of May, 1980\")my_birthdayTimestamp('1980-05-28 00:00:00')DatetimeIndex : DataFrame이나 Series의 인덱스로 사용되는 포맷을 구성한다.dates = [datetime(2022, 1, 9), datetime(2022, 1, 9)+timedelta(days= 20)]dt_index = pd.DatetimeIndex(dates)dt_index    DatetimeIndex(['2022-01-09', '2022-01-29'], dtype='datetime64[ns]', freq=None)특정 구간을 DatetimeIndex로 만들려면 시작, 종료지점, 그리고 빈도수를 지정하면 된다.dt_index = pd.date_range(dates[0], dates[1], freq='D')dt_index    DatetimeIndex(['2022-01-09', '2022-01-10', '2022-01-11', '2022-01-12',                   '2022-01-13', '2022-01-14', '2022-01-15', '2022-01-16',                   '2022-01-17', '2022-01-18', '2022-01-19', '2022-01-20',                   '2022-01-21', '2022-01-22', '2022-01-23', '2022-01-24',                   '2022-01-25', '2022-01-26', '2022-01-27', '2022-01-28',                   '2022-01-29'],                  dtype='datetime64[ns]', freq='D')또 다른 방법dt_index = dates[0] + pd.to_timedelta(np.arange(21), unit='D')dt_index    DatetimeIndex(['2022-01-09', '2022-01-10', '2022-01-11', '2022-01-12',                   '2022-01-13', '2022-01-14', '2022-01-15', '2022-01-16',                   '2022-01-17', '2022-01-18', '2022-01-19', '2022-01-20',                   '2022-01-21', '2022-01-22', '2022-01-23', '2022-01-24',                   '2022-01-25', '2022-01-26', '2022-01-27', '2022-01-28',                   '2022-01-29'],                  dtype='datetime64[ns]', freq=None)freq 관련 주요 인자는 다음과 같다.  s: 초  T: 분  H: 시간  D: 일(day)  B: 주말이 아닌 평일  W: 주(일요일)  W-MON: 주(월요일)  M: 각 달(month)의 마지막 날  MS: 각 달의 첫날  BM: 주말이 아닌 평일 중에서 각 달의 마지막 날  BMS: 주말이 아닌 평일 중에서 각 달의 첫날  WOM-2THU: 각 달의 두번째 목요일  Q-JAN: 각 분기의 첫달의 마지막 날  Q-DEC: 각 분기의 마지막 달의 마지막 날date_range와 유사한 period_range, timedelta_range도 있다.pd.period_range('2022-01', periods=8, freq='M')    PeriodIndex(['2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06',                 '2022-07', '2022-08'],                dtype='period[M]', freq='M')pd.timedelta_range(0, periods=10, freq='H')    TimedeltaIndex(['0 days 00:00:00', '0 days 01:00:00', '0 days 02:00:00',                    '0 days 03:00:00', '0 days 04:00:00', '0 days 05:00:00',                    '0 days 06:00:00', '0 days 07:00:00', '0 days 08:00:00',                    '0 days 09:00:00'],                   dtype='timedelta64[ns]', freq='H')pd.timedelta_range(0, periods=10, freq='1T30S') # 1T30S : 1분 30초    TimedeltaIndex(['0 days 00:00:00', '0 days 00:01:30', '0 days 00:03:00',                    '0 days 00:04:30', '0 days 00:06:00', '0 days 00:07:30',                    '0 days 00:09:00', '0 days 00:10:30', '0 days 00:12:00',                    '0 days 00:13:30'],                   dtype='timedelta64[ns]', freq='90S')4. 시계열 데이터셋 만들기dates = [datetime(2022, 1, 1), datetime(2022, 1, 1)+timedelta(days= 4)]dt_index = pd.date_range(dates[0], dates[1], freq='D')Series 만들기series = pd.Series(np.random.randn(len(dt_index)), index=dt_index)series    2022-01-01   -0.132021    2022-01-02    1.090681    2022-01-03   -0.787756    2022-01-04   -2.461332    2022-01-05   -0.315497    Freq: D, dtype: float64Dataframe 만들기columns = ['A', 'B', 'C', 'D']data = np.random.randn(len(dt_index), len(columns))df = pd.DataFrame(data = data, index=dt_index, columns=columns)df                  A      B      C      D                  2022-01-01      0.887873      -1.127382      2.203209      -0.247588              2022-01-02      -0.653866      0.350450      0.089382      -2.678953              2022-01-03      -0.976342      0.032952      0.675049      -0.308314              2022-01-04      1.318794      0.667468      -0.886021      2.705370              2022-01-05      0.302409      -0.712497      0.513426      2.089435      Dataframe 살펴보기df.index    DatetimeIndex(['2022-01-01', '2022-01-02', '2022-01-03', '2022-01-04',                   '2022-01-05'],                  dtype='datetime64[ns]', freq='D')df.index.min()    Timestamp('2022-01-01 00:00:00', freq='D')df.index.max()    Timestamp('2022-01-05 00:00:00', freq='D')  참고자료1 : https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html  참고자료2 : https://tacademy.skplanet.com/live/player/onlineLectureDetail.action?seq=152",
        "url": "/ds4"
    }
    ,
    
    "it05": {
        "title": "오프라인 환경에 파이썬 개발환경 구축하기",
            "author": "km.yu99",
            "category": "",
            "content": "기업에서의 SW 개발환경은 보안 등의 이슈로 인해 인터넷으로부터 단절되어 있는 경우가 있다. 인터넷이 되더라도 특정 사이트가 방화벽으로 차단되어 있어 pip나 conda install이 원활하지 못할 수도 있다. 본 포스트에서는 오프라인 개발 환경에서 파이썬 개발환경을 구축하는 방법에 대해서 소개한다.윈도우OS, Minicona를 환경을 기준으로 설명한다.준비사항은 다음과 같다.  인터넷이 연결된 PC : 설치파일 및 파이썬 패키지 다운로드용, pip 인스톨러가 설치되어있다고 가정  오프라인 PC인터넷이 연결된 PC에서 설치파일과 패키지를 다운로드 받고, 이 것들을 오프라인 PC(개발환경)에 복사하여 설치를 진행하는 방식이다.1. Miniconda 다운로드 및 설치Anaconda는 데이터분석 패키지 관리와 사용을 용이하게 하는 어플리케이션이다. 개인이나 학생 등의 연구자는 Individual Edition을 사용하면 된다. Anaconda로 150여개의 패키지들이 함께 설치되므로 편리하다.2020년 하반기 유료버전(Commercial edition)에 대한 기준이 새롭게 설정되었다. 즉 200명 이상의 종업원을 보유한 기업이나 영리기관은 유료버전을 사용해야 한다.[참고]유료버전을 사용하기 곤란한 상황이라면 대안이 있다. 바로 Miniconda를 사용하는 것이다. Minicona는 Anaconda의 minimal 버전으로 무료이다. 파이썬과 패키지 관리자 프로그램이 기본 제공된다. 다만 데이터 분석에 사용되는 패키지들은 포함되어 있지 않으므로 직접 설치해야한다. 회사에서 무료버전을 사용해야하거나 Anaconda에서 제공되는 라이브러리 전체를 사용하지 않아서 가벼운 환경을 원한다면 Miniconda가 대안이 될 수 있다.이제 설치를 위해 공식문서 url에 접속한다.  https://docs.conda.io/en/latest/miniconda.html여기서는 윈도우 OS를 기준으로 설명한다. 사용자의 OS, 파이썬 버전에 맞는 인스톨러를 다운로드 받는다.이 파일을 오프라인 PC에 복사해서 설치한다.윈도우 OS의 시작버튼을 클릭해서 Anaconda Prompt가 설치되어 있으면 정상 설치된 것이다.2. 파이썬 패키지 다운로드 받기이제 오프라인 pc에 설치할 패키지를 다운도르 받을 차례다.인터넷이 연결된 pc에서 패키지 다운로드 명령은 다음과 같은 형태이다.  pip download 패키지명==버전번호예를 들어 numpy 1.19.5를 다운로드 받고 싶다면 다음과 같이 입력하면 된다. 버전번호를 입력하지 않으면 최신 버전이 저장된다.pip download numpy==1.19.5우선 패키지들을 저장할 폴더를 생성하고 진입하자. (여기서는 pkg라는 이름의 폴더를 생성하였다.)(base) C:\\&gt;mkdir pkg(base) C:\\&gt;cd pkg(base) C:\\pkg&gt;그리고 앞에서 설명한 pip download 명령을 사용해서 사용할 패키지들 다운로드 받는다. 패키지 사용에 필요한 dependency도 함께 저장된다.(base) C:\\pkg&gt;pip download numpy==1.19.5Collecting numpy==1.19.5  Using cached numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)Saved c:\\pkg\\numpy-1.19.5-cp38-cp38-win_amd64.whlSuccessfully downloaded numpy패키지를 다운로드 하면서 폴더안에 requirements.txt파일을 생성하여 저장한 패키지명과 정보를 기록한다.다음과 같은 형태로 기록하면 된다.numpy==1.19.5pandas==1.2.4matplotlib==3.3.4lightgbm==3.2.1scikit-learn==0.24.1seaborn==0.11.1joblib==1.0.1jupyter필요한 설치가 완료되면 폴더안에 requirements.txt 파일과 whl 확장자 파일이 저장되어 있을 것이다.참 고 : 기존 환경에 설치된 패키지리스트를 다운로드 받고 싶다면?pip freeze 명령은 현재 (가상)환경에 설치된 전체 패키지를 출력한다.(base) C:\\&gt;pip freezeabsl-py==0.14.1alabaster @ file:///home/ktietz/src/ci/alabaster_1611921544520/workanaconda-client==1.7.2anaconda-navigator==2.0.3anaconda-project @ file:///tmp/build/80754af9/anaconda-project_1610472525955/workanyio @ file:///C:/ci/anyio_1620153418380/work/distappdirs==1.4.4argh==0.26.2---(생 략)패키지 리스트를 requirements.txt 파일에 출력하고 싶다면 다음과 같이 입력한다.(파일에 기록할 때는 Anaconda prompt를 관린자 권한으로 실행해야 한다.)(base) C:\\pkg&gt;pip freeze &gt; requirements.txt이파일에 기록된 패키지를 다운로드 하는 명령은 다음과 같다.(base) C:\\pkg&gt;pip download -r requirements.txtCollecting numpy==1.19.5  Downloading numpy-1.19.5-cp39-cp39-win_amd64.whl (13.3 MB)     |████████████████████████████████| 13.3 MB 6.4 MB/sCollecting pandas==1.2.4  Downloading pandas-1.2.4-cp39-cp39-win_amd64.whl (9.3 MB)     |████████████████████████████████| 9.3 MB ...Collecting matplotlib==3.3.4  Downloading matplotlib-3.3.4-cp39-cp39-win_amd64.whl (8.5 MB)     |████████████████████████████████| 8.5 MB 6.4 MB/s---(생 략)3. 다운로드 받은 패키지 설치하기앞에서 다운로드 받은 패키지 폴더 전체를 오프라인 pc에 복사한다.Miniconda의 Anaconda prompt를 실행한다.폴더 안으로 이동하여 오프라인 패키지를 설치하는 명령을 다음과 같이 입력한다.(base) C:\\pkg&gt; pip install --no-index --find-links=\"./\" -r requirements.txtLooking in links: ./Processing c:\\pkg\\numpy-1.19.5-cp39-cp39-win_amd64.whlProcessing c:\\pkg\\pandas-1.2.4-cp39-cp39-win_amd64.whlProcessing c:\\pkg\\matplotlib-3.3.4-cp39-cp39-win_amd64.whlProcessing c:\\pkg\\lightgbm-3.2.1-py3-none-win_amd64.whlProcessing c:\\pkg\\scikit_learn-0.24.1-cp39-cp39-win_amd64.whl---(생 략)설치가 완료되었다. 설치 후에는 패키지 폴더를 삭제해도 된다.",
        "url": "/it05"
    }
    ,
    
    "story03": {
        "title": "나의 이직 이야기 - 2",
            "author": "km.yu99",
            "category": "",
            "content": "- 임소장1 -2018년부터 나의 직함은 데이터사이언티스트가 되었다.사내 데이터사이언티스트 공모에 지원했고, 덜컥 서류-면접을 통해 합격했다.(지원 자격요건은 ‘데이터사이언스와 머신러닝에 호기심과 열정이 큰 직원’ 이었다.)정부출연연구소나 공기업의 연구소의 운영 방식은 국회나 공무원 조직의 그것과 비슷하다.조직의 장을 배치할 때 과거 이력이나 전문성 보다는, 그 사람의 지위나 영향력을 우선 고려한다.장은 1, 2년 근무 후 타 부서 또는 다른 보직으로 이동해야 한다.따라서 관리자들이 전문성을 유지하기 어렵고, 어짜피 1, 2년 후에는 이동하기 때문에 조직 운영에 대한 열정과 책임감도 약하다.조직원 구성도 마찬가지다. 외부 전문가 채용에 인색한 편이다. 주로 내부 인원 차출, 재배치로 팀을 꾸린다.유행에 따라 연구개발 테마도 자주 바뀐다. 선진국의 기술 트렌드를 쫒기 바쁘다.미국, 독일, 일본에서 관련 연구를 시작했고 가시적인 성과가 나왔는지가 중요하다.2030, 2050을 타겟으로 하는 중장기 로드맵은 있지만 매번 유행만 좆다보니 너덜너덜해졌다.잦은 정책 변화 때문에 연구원들은 매년 책상 배치를 바꾸고 사무실을 이리저리 옮기는 조직개편 행사를 경험한다.사람은 그대로다. 사무실 명패와 책상 배치만 달라질 뿐.어쨌는 ‘한국형’ AI, ‘한국형’ 빅데이터 기술을 선도한다는 미명 아래,나를 포함한 사내 공모 인력 다섯명은 빅데이터 전문가가 되었다.이번에는 다를거라고 한다. 소장될분이 외부 전문가로 특별채용했고, 우리를 제외한 나머지 실무자들도 전공자로 영입될 계획이다.우리는 1월 1일 부로 서울 데이터사이언스 연구소로 발령을 받았다.연구소 사무실에는 30개 책상만 덩그러니 놓여 있었다. 앞으로 입사할 신입사원들 자리를 포함한 것이다.책상만 제공 받았을 뿐, 관리자도 없고 행정 지원도 없다. 우리가 다 알아서 해야한다.회사의 지원은 여기까지다. 정수기부터 인터넷 연결까지 알아서 처리해야 했다.사무실에는 사외 전문가로 영입된 박사 인력 두명이 한달 전부터 근무하고 있었다.우리들과 인사를 나눴지만 서로간에 미묘한 긴장감이 흐른다.전문가 두분을 간단히 소개 하자면,      오박사(55세) : 도쿄대 경영학 박사. 나이가 많음…. 과거 경력이 빅데이터와 어떤 연관성이 있는지 알려져 있지 않음. 15개 이상의 기업에서 근무. 회사별 근속연수 평균 1.5년.        박박사(39세) : 통계학 박사. 우리 중 유일하게 학교에서 데이터 관련 기술을 전공한 사람.  첫날인데 조직의 리더인 연구소장이 보이지 않는다.소장은 MIT에서 AI로 박사학위를 받고 국내 글로벌 대기업 S -&gt; L -&gt; S를 거쳐우리회사에 스카웃된 데이터사이언스 최고 전문가라고 했다.공공기관은 외부 인재 영입에 상당히 보수적이다.위로 갈수록 보직자리가 줄어드는데 능력있는 사람이 오는 것을 반기지 않는다.외부에서 인재를 채용하고 보직까지 부여했다는 것은 상당히 파격적인 인사였다.당연히 전사 직원들의 관심이 몰렸다.소장이 없는 동안 우리끼리 알아서 사무환경을 구축하고 연구소 운영계획을 세웠다.인터넷 라인과 PC를 구매해서 사내 시스템에 연결했고, 세부조직을 어떻게 구성할지 의논했다.잘나간다는 G사, N사, 그리고 ‘카’사 연구소를 방문해서 조직이 어떻게 운영되어야 할지 벤치마킹했다.소장은 2주가 지나도 출근하지 않는다.채용을 알선한 헤드헌터를 통해 한달 뒤에 입사하겠다고 통보했다고 한다.그의 입사 날이 가까워짐에 따라 궁금증은 더 커져간다.식사 시간이나 티타임 대화의 단골 주제는 소장에 관한 이야기였다.‘MIT 박사면 우리가 일하는 모습에 답답해하지 않을까요? 초반부터 찍히면 안되는데…’‘신임소장 이름이 임xx 래요. 대단한 사람이라고 하는데 인터넷 검색하면 정보가 하나도 안나와요.’테이블 끝에 앉아있던 구차장이 한마디 꺼낸다.‘내가 본사 담당자 한테 들은게 있는데…소장, 신혼여행 가야한다고 입사 한달 연기해달라고 했대요. 안그러면 입사 안하겠다고…’평범한 성격은 아닌 것 같다.",
        "url": "/story03"
    }
    ,
    
    "foody6": {
        "title": "(경기 수원 맛집) 수원만두",
            "author": "km.yu99",
            "category": "",
            "content": "수원 행궁 인근 중식당 수원만두  주 소 :  경기 수원시 팔달구 창룡대로8번길 6  영업시간 : 11:30 ~ 21:00  전화번호 : 031-255-5526  대표메뉴 : 쇠고기탕면(7,000원), 군만두/찐만두(7,000원), 볶음면(7,000원)10월에 방문하고 이제서야 포스팅하게 되었다.수원화성 인근은 가족 동반으로 나들이 떠나기 좋은 곳이다. 특히 연무대는 날씨가 좋으면 돚자리를 깔고 피크닉하기도 좋고 성벽을 따라 산책하기에도 안성맞춤이다.이날은 날씨가 추워지기 전에 마지막 피크닉을 즐기려는 사람들로 분주했다. 도시락을 싸와서 음식을 먹는 사람도 많았고 근처 가게에서 연을 사와서 날리는 사람들도 있었다.한가지 아쉬운 점은 연무대 근처에 마땅한 식당이 없다는 점이다. 아무 것도 준비해오지 않은 우리는 식사를 하기 위해 차로 이동해야 했다.식당가는 화성행궁에서 팔달문 까지 이어지는 거리에 집중되어 있다. 그런데 이 쪽 방면은 나들이 객들로 인해 주차공간이 부족했다. 화성행궁 주차장은 이미 만차로 대기열이 수십미터는 되어 보였다. 차선 책으로 팔달구청 주차장으로 이동해서 겨우 한개 남은 자리에 주차를 하고 이동했다.이날 방문한 수원만두는 아쉽게도 주차공간이 없다. 팔달구청 주차장에서 도보로 10 ~ 15분 소요되는 곳에 위치하고 있다. 점심때를 넘긴 오후 2시에 도착했지만 아직도 가게 앞에 다섯팀 정도가 대기하고 있다. 입구가 협소해서 마땅히 앉을 곳이 없으니 가게 앞에 서서 대기해야 한다.30분을 기다려서 자리를 잡았다. 내부는 중국 노포식당 분위기가 물씬 풍긴다. 화교분들이 직접 운영하는 가게로 일반 중국집과 메뉴 구성도 다르다. 가게 이름에 만두가 들어가지만, 그 보다 더 맛보고 싶은 음식이 있었다. 쇠고기탕면. 오기 전에 검색한 블로그에서 극찬하던 메뉴다. 주변을 둘러보니 잡탕밥도 많이 주문하는 것 같아서 함께 주문해본다.붉은 색을 띈 맑은 국물의 쇠고기탕면이 나온다. 기대했던 비주얼과는 약간 차이가 있었다. 국물을 한숫갈 떠보았다. 기름지지 않아 오히려 부담스럽지 않았다. 칼칼한 국물의 깊은 맛이 인상적이다. 생각보다 양이 많지 않아서 아쉬웠지만 7000원 가격을 생각하면 가성비는 좋은편.이어 나온 잡탕밥. 굵직한 해산물이 얹혀져 나온다. 고소한 소스맛도 일품이다. 일행들의 숫가락이 몇번 오가니 금새 바닥을 보인다.온김에 이집의 주력 메뉴인 군만두 한접시도 주문했다. 만두를 일렬로 붙여서 튀겨서 나온다. 사실 만두를 좋아하는 편은 아니라서 평가를 잘 못하겠다. 만두피는 약간 두껍고 바삭하게 튀겨져서 색다른 맛임은 확실하다.허겁지검 먹다보니 사진을 제대로 남기지 못했다. 주로 가족 단위 손님이 많은 것으로 보이는데 이 근방에서도 맛집으로 통하는 것 같다.정통 중국 노포식당의 맛을 경험하고 싶다면 방문해보자. 주차공간이 아쉽고 대기를 해야하는 것이 부담스럽긴 한데, 가격과 음식의 질을 생각한다면 방문해볼만한 가치가 있다.총 평  음식맛 : ★★★★☆  가성비 : ★★★★☆  서비스 : ★★★☆☆  접근성 : ★★★☆☆",
        "url": "/foody6"
    }
    ,
    
    "story02": {
        "title": "나의 이직 이야기 - 1",
            "author": "km.yu99",
            "category": "",
            "content": "- 소박한 환송회 -공식적인 퇴사 후 일주일이 지났다.같이 일했던 동료들이 나를 불러냈다. 그동안 공식적인 환송회는 없었다.소장과 팀장은 불쑥 사직서를 내민 내가 괘씸했을 것이다.팀원들이 식사자리 없이 내보낸게 마음에 걸렸는지 윗사람들 몰래 따로 불렀다.형동생하던 술동무 몇명만 모였다. 사무실 옆 단골 통닭집으로 줄지어 들어간다.‘삼촌 어디갔었노? 와이리 오랜만에 오노?’이틀에 한번꼴로 가던 가게에 열흘 넘게 안비췄더니 사장님이 반기신다.익숙하게 냉장고에서 소주를 꺼낸다. 사장님이 바쁘면 우리가 알아서 기본 세팅을 한다.안주가 나오기 전, 빈속에 소주 한잔을 들이켰다.‘명이 너는 나가서도 잘 할꺼야. 걱정마라.’‘그니까 임마, 옮길 회사 정해놓고 나갔어야지. 뭐하는거야, 정신나간 놈아.’취기가 오르니까 욕이 오고간다.다 잘되라고 하는 말인 것을 알기 때문에 기분이 나쁘진 않다.소주가 달다.아니, 선배들의 직언에 몸둘바를 몰라 쓴맛을 느낄 틈이 없다.‘어디라도 합격하고 나간거야?’‘S 하나랑 스타트업 합격했는데, 그것들은 보험이고 일단 이것저것 알아보려구요. 논문도 좀 쓰고, 홈페이지도 만들고…’‘시끄러 임마. 일단 어디라도 빨리 붙어서 들어가. ‘사실 두 회사 모두 1차면접만 통과한 상태다. 최종합격 통보를 준 곳은 없다.대책없는 놈으로 보이기 싫어서 이리저리 둘러댄다.이직 확정 후 퇴사를 하는 것이 정석이라지만, 회사를 하루라도 더 다니면 정신이 나갈 것 같았다.‘자, 서비스 안주 나왔습니다.’‘벌써 10시냐? 사장님 서비스만 먹고 뜨자. 명아, 가끔 연락하고 지내자. 우린 내일 출근해야 하니까 일어날게’사장님은 손님이 뜸한 10시가 되면, 메뉴판에 없는 안주를 만들어 서비스로 주곤 하신다.단골인 우리는 서비스 안주가 나오면 10시가 된 것을 눈치챈다.가게를 나왔지만 다들 아쉬웠는지 입구에 모여 담배를 물고 한참 수다를 떤다.‘아무리 그래도 소장하고 부장이 먼저 환송회 하자고 해야하는거 아니냐?’‘에이, 그 사람들이 잘도 하겠다. 이제 남이라는 거지 뭐’‘괜찮아요. 저는…’임소장과 최부장.퇴사 결정과 연관이 없다고 말해왔지만, 주변에서 언급 할 때마다 신경 쓰인다.집으로 가는 택시 안에서 관계가 언제, 어디서부터 삐걱이기 시작했는지 곱씹어본다.  회사 근처에는 직원들끼리 모여 식사를 할 장소가 별로 없었다.유일한 장소가 이 치킨집이었다.최근에 재개발로 인해 이 곳 마저도 문을 닫았다.",
        "url": "/story02"
    }
    ,
    
    "story01": {
        "title": "나의 이직 이야기 - 프롤로그",
            "author": "km.yu99",
            "category": "",
            "content": "2020년 6월 30일.10년간 다니던 회사에 사직서를 제출했다.이제 구직자 신세가 되었다.이렇게 대책없이 회사를 그만 둘 것이라고는 상상도 못했다.회사를 그만두는 과정은 그야말로 충동적이었다.퇴사 결심부터 회사에 알리기까지 1주일도 안걸렸다.부서장께 퇴직 의사를 밝히고, 동료들의 회유가 한창일때도 몰랐다. 내가 무슨짓을 저지르고 있는지.‘너 나가면 우리는 어떡하냐’, ‘멋지다, 부럽다’  걷치레인줄 알면서도 동료들의 한마디를 들으면 뭔지 모르게 뿌듯했다.며칠 휴가를 낸 후, 퇴사일이 되서 사무실에 인사하러 들렀다.딱히 날 잡는 사람은 없다. 아무일 없었다는 듯 다들 분주하다.서운한 감정도 없잖았지만 웃으며 다음을 기약한다.사무실에 있던 짐들을 담은 박스를 들고 대로를 터벅터벅 걸으며 생각에 잠긴다.내가 지금 뭘 한거지? 제대로 하고 있는거지?회사의 울타리를 갓 벗어나서야 보이기 시작한다.회사 안은 따뜻한 온돌방이다. 한겨울에 밖이 얼마나 추운지 잘 모른다. 딱 그 느낌.조직에 몸담고 있을 때는 사무실이 감옥 같았다. 문을 박차고 나가야 창창한 앞날이 펼쳐질 것만 같았다.정신이 번쩍 든다. 문틈으로 보이는 빛들은 그저 미지에 대한 호기심이었다.아무런 준비도 없이 일을 저질러 버렸다.이제 서야 현실의 찬바람이 느껴지기 시작한다.",
        "url": "/story01"
    }
    ,
    
    "ds3": {
        "title": "MySQL server 설치하기 - windows편",
            "author": "km.yu99",
            "category": "",
            "content": "본 포스트에서는 MySQL server 설치 방법에 대해서 다룬다. MySQL은 널리 사용되는 관계형 DB 솔루션 중 하나이다. 여기서는 windows os 기반으로 설명한다.1. MySQL Installer 다운로드아래 다운로드 주소에 접속한다.  https://dev.mysql.com/downloads/mysql/OS가 Microsoft Windows로 설정된 것을 확인한 후, Go to Download Page 버튼을 클릭한다. 이후 상황에 맞는 installer를 선택하여 Download 버튼을 클릭한다. 여기서는 local installer를 선택하였다. (로그인 화면으로 전환되면 로그인하여 진행. 또는 회원 가입 후 진행)2. MySQL Installer 설치다운 받은 installer 파일을 실행한다.용도에 따른 Setup Type을 지정한다. 여기서는 개발자용인 Developer Default를 선택하고 다음(Next)로 이동하였다.설치할 항목이 표시되면 리스트를 확인한 후 Execute버튼을 선택한다. 설치가 진행될 것이다.3. Product configuration설치가 완료된 후 Next 버튼을 클릭하면 설정항목으로 이동한다.3.1 Type and Networking, Authentication MethodConfig Type을 용도에 맞기 설정한다.(여기서는 Development Computer) Connectivity 항목에서는 서버 연결에 사용할 포트를 지정한다. default 값은 3306이다. 만약 해당 포트가 사용중이면 다른 값을 지정한다.Next를 눌러 Authentication Method를 지정한다. RECOMMENDED 설정으로 두고 Next 버튼을 선택한다.3.2 Accounts and Roles, Windows Service Nameroot 계정, 즉 최상위 권한 계정의 패스워드를 설정한다. 다음으로 이동하여 Windows Service를 설정한다. OS 부팅 후 자동으로 MySQL이 재시작 하도록, Start the MySQL Server at System Startup 항목을 체크한다.3.3 Connect To ServerNext 버튼을 선택하여 설정을 이어 나간다. Connect To Server 항목에서 앞에 설정한 패스워드를 입력하여 연결 시험을 해본다. 상단의 Status 항목에 Connection succeeded 문구가 표시되면 정상 연결되는 것이다.이후 Next를 선택하여 설치를 완료한다.4. MySQL Workbench 실행하기MySQL Workbench는 MySQL에 접근하기 위한 GUI 기반 프로그램이다. 앞에서 개발자용(Developer)으로 설치를 진행하였다면 함께 설치된다. 별도 설치를 하려면 다음 경로에 접속하여 설치한다.Workbench를 실행하여 root 계정으로 로그인한다.로그인이 되면 정상 접근이 되는 것이다.마지막으로 Edit-Preferences를 선택한다. 설정 창 좌측에서 SQL Editer 항목을 선택한다. 그리고 우측에 Safe Updates 항목을 해제한다.Safe Updates 기능이란, 테이블의 넓은 범위를 update나 delete를 하려고 할때 에러 메시지를 발생시켜 방지하는 일종의 안전모드이다. 그대로 두고 싶다면 해제하지 않아도 된다.참 고MySQL Server 접근은 Workbench 뿐만 아니라 command 창에서도 가능하다. 예를 들어 root 계정으로 로그인 하고자 한다면 커맨드 창에서 다음과 같이 입력한다.mysql -u root -p",
        "url": "/ds3"
    }
    ,
    
    "it04": {
        "title": "jekyll 블로그에 utterance로 댓글기능 적용하기",
            "author": "km.yu99",
            "category": "",
            "content": "jekyll 블로그에서는 댓글 기능을 제공하지 않는다. 따라서 별도로 댓글 서비스를 적용해야한다.자주 사용되는 댓글 서비스 중에 Disqus라는 것이 있다.  본인도 한동안 Disqus를 사용해왔다. 그런데 Disqus 무료 라이센스를 사용하면 블로그에 광고가 과도하게 붙는다. 그리고 서비스도 무거운 편이다.본 포스트에서는 또다른 댓글 관리 서비스인 utterances를 jekyll 블로그에 설정하는 방법을 소개한다. github에 issue를 활용하여 댓글을 관리하는데 광고도 붙지않고 가벼워서 개인 블로그 운영에 적합하다.1. 댓글 관리용 저장소 생성하기우선 개인 github 계정에 댓글들을 관리할 저장소를 생성한다. 본인의 경우에는 blog-comments라는 이름으로 생성하였다. public으로 생성한다.2. Utterances app 설치 및 설정다음 경로로 들어가서 Inatall 버튼을 클릭하여 utterences app을 설치한다.  https://github.com/apps/utterances설치할 저장소를 지정하는 부분이 표시되면, 앞에서 생성한 저장소 이름을 입력하면 된다.(여기서는 blog-comments) 그리고 Install 버튼을 클릭한다.계속해서 설정항목으로 이동한다.Repository 항목에서 저장소 정보를 설정한다. 계정이름/저장소이름 형태로 입력하면 된다. 여기서는 sguys99/blog-comments로 입력하였다.바로 아래 포스트와 github 저장소 간 댓글을 매핑하는 방법을 설정하는 항목이 있다. 개인 선호에 따라 선택하면 된다. 여기서는 Issue title contains page pathname을 선택하였다.선호하는 테마를 선택한다. 여기서는 Github Light를 선택하였다.모든 설정이 완료되면 맨아래에 설정을 반영한 script가 자동으로 생성된다. 이 부분을 복사해둔다.3. Jekyll 블로그에 설정 반영하기이제 설정한 내용을 블로그에 소스에 반영한다. 본인은 jasper2 템플릿을 사용한다. _layouts/post.html  파일은 블로그의 포스트 구성을 정의한 파일이다. 이 파일에 앞에 복사한 내용을 추가해주면 된다.jasper2 의 경우 Disqus를 기본 댓글 서비스로 사용하도록 작성되어 있다. 이와 관련된 부분을 주석처리한다.그리고 이 부분에 앞에서 복사한 script를 붙여 넣어준다.4. 시험해보기블로그 하단에 다음과 같은 댓글 항목이 나타나면 정상 동작하는 하는 것이다. 시험삼아 댓글을 입력해보자.입력된 댓글들은 생성한 저장소의 Issues 항목에서 관리할 수 있다.",
        "url": "/it04"
    }
    ,
    
    "trading03": {
        "title": "네이버에서 일별 시세 가져오기",
            "author": "km.yu99",
            "category": "",
            "content": "네이버에서는 국내 주식과 관련된 다양한 정보를 제공한다. 본 포스트에서는 네이버 금융에서 일별 주식 시세를 가져오는 방법을 소개한다.1. 홈페이지 둘러보기네이버 금융에 접속한다.  https://finance.naver.com/여기서는 삼성전자를 예로 설명한다. 종목명에 삼성전자를 입력하고 검색버튼을 클릭한다.아래에 시세 버튼을 클릭하면 시간별시세와 일별시세를 확인할 수 있다.우리의 관심항목은 일별시세이다.삼성전자 시세 항목의 url도 살펴보자.  https://finance.naver.com/item/sise.naver?code=005930get 방식으로 서버에 시세정보를 요청하는 형태이다. 파라미터로 종목코드를 사용한다.(삼성전자의 종목코드는 005930)크롬에서 일별시세 부분을 마우스 우클릭하고 검사를 클릭한다.개발자 도구 창이 열리면 Network 탭을 선택하자.일별 시세 아래에 2번 페이지를 클릭해본다. sise_day.naver?code=005930&amp;page=2라는 항목이 새로 생성되었다.이 항목의 헤더를 살펴보면 일별 시세를 가져오고자 하는 서버 url과 파라미터, 그리고 요청 방식을 확인할 수 있다.  서버 주소 : https://finance.naver.com/item/sise_day.naver  파라미터 : code, page  요청방식 : GET사실 get방식으로 url에 입력하면 일별 시세 항목을 조회할 수 있다.예를 들어 삼성전자(005930) 시세의 첫번째 페이지를 조회하고 싶다면 다음과 같이 url을 작성하여 입력하면 된다.  https://finance.naver.com/item/sise_day.naver?code=005930&amp;page=1전체 페이지에 기록된 시세를 가져오려면 마지막 페이지를 번호를 알아야 한다.마우스로 맨뒤 항목을 가리킨 후, 마우스 우클릭-검사를 선택하자.마지막 페이지의 url은 pgRR라는 클래스의 td 태그 내에 정의되어 있다.  https://finance.naver.com/item/sise_day.naver?code=005930&amp;page=641즉, 마지막 페이지 번호는 641이다.이로써 일별 시세를 가져오기 위한 확인과정은 모두 끝났다.2. 일별시세 추출하기여기서는 requests와 BeautifulSoup을 사용해서 추출하는 방법을 설명한다.import pandas as pdfrom bs4 import BeautifulSoupimport requestsfrom datetime import datetime2.1 마지막 페이지 번호 찾기먼저 마지막 페이지를 찾는 방법을 설명한다.code = '005930' # 삼성전자 종목코드url = f\"http://finance.naver.com/item/sise_day.nhn?code={code}\"headers = {'User-agent': 'Mozilla/5.0'} # 웹브라우저 접속처럼 인식시키기 위해 정보 추가서버 주소에 종목 코드를 추가하여 get 양식을 완성하고 요청한다.req = requests.get(url=url, headers = headers)req.text'\\n&lt;html lang=\"ko\"&gt;\\n&lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=euc-kr\"&gt;\\n&lt;title&gt;네이버 금융&lt;/title&gt;\\n\\n&lt;link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/imgstock/static.pc/20211216210327/css/newstock.css\"&gt;\\n&lt;link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/imgstock/static.pc/20211216210327/css/common.css\"&gt;\\n&lt;link ----(생 략)BeautifulSoup으로 추출한 내용을 정리한다.bs = BeautifulSoup(req.text, 'html.parser')bs    &lt;html lang=\"ko\"&gt;    &lt;head&gt;    &lt;meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/&gt;    &lt;title&gt;네이버 금융&lt;/title&gt;    &lt;link href=\"https://ssl.pstatic.net/imgstock/static.pc/20211216210327/css/newstock.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;        --- (생 략)앞에서 마지막 페이지의 url은 pgRR라는 클래스의 td태그 내에 정의되어 있는 것을 확인했다.이 정보를 바탕으로 마지막 페이지를 찾을 수 있다.pgrr = bs.find('td', class_='pgRR')print(pgrr)    &lt;td class=\"pgRR\"&gt;    &lt;a href=\"/item/sise_day.nhn?code=005930&amp;amp;page=641\"&gt;맨뒤    \t\t\t\t&lt;img alt=\"\" border=\"0\" height=\"5\" src=\"https://ssl.pstatic.net/static/n/cmn/bu_pgarRR.gif\" width=\"8\"/&gt;    &lt;/a&gt;    &lt;/td&gt;pgrr.a[\"href\"].split('=')['/item/sise_day.nhn?code', '005930&amp;page', '641']last_page = int(pgrr.a[\"href\"].split('=')[-1])last_page6412.2 일별시세 추출하기일별 시세는 http://finance.naver.com/item/sise_day.nhn에 code와 page 정보를 추가하여 get으로 요청하면 된다.우선 첫번째 페이지만 추출해보자.page_url = '{}&amp;page={}'.format(url, 1)page_url'http://finance.naver.com/item/sise_day.nhn?code=005930&amp;page=1'data = pd.read_html(requests.get(page_url, headers={'User-agent': 'Mozilla/5.0'}).text)[0]data                  날짜      종가      전일비      시가      고가      저가      거래량                  0      NaN      NaN      NaN      NaN      NaN      NaN      NaN              1      2021.12.27      80200.0      300.0      80600.0      80600.0      79800.0      10751648.0              2      2021.12.24      80500.0      600.0      80200.0      80800.0      80200.0      12086380.0              3      2021.12.23      79900.0      500.0      79800.0      80000.0      79300.0      13577498.0              4      2021.12.22      79400.0      1300.0      78900.0      79400.0      78800.0      17105892.0              5      2021.12.21      78100.0      1000.0      77900.0      78300.0      77500.0      14245298.0              6      NaN      NaN      NaN      NaN      NaN      NaN      NaN              7      NaN      NaN      NaN      NaN      NaN      NaN      NaN              8      NaN      NaN      NaN      NaN      NaN      NaN      NaN              9      2021.12.20      77100.0      900.0      77600.0      77800.0      76800.0      11264375.0              10      2021.12.17      78000.0      200.0      76800.0      78000.0      76800.0      13108479.0              11      2021.12.16      77800.0      200.0      78500.0      78500.0      77400.0      11996128.0              12      2021.12.15      77600.0      600.0      76400.0      77600.0      76300.0      9584939.0              13      2021.12.14      77000.0      200.0      76500.0      77200.0      76200.0      10976660.0              14      NaN      NaN      NaN      NaN      NaN      NaN      NaN      주말이나 휴일 같이 장이 열리지 않은 날은 null 값이 들어가있다.이제 10개 페이지를 추출해보자. 전체 페이지 수와 10을 비교해서 작은 값을 추출할 페이지 수(pages)로 지정한다.page_no = 10pages = min(last_page, page_no) # 마지막 페이지와 가져올 페이지 수 중에 작은 값 선택루프를 돌면서 각 페이지의 일별 시세를 추출하여 병합한다.df = pd.DataFrame()for page in range(1, pages+1):    page_url = '{}&amp;page={}'.format(url, page)    df = df.append(pd.read_html(requests.get(page_url, headers={'User-agent': 'Mozilla/5.0'}).text)[0])추출한 시세의 컬럼명을 수정하고, 데이터 타입 변경, 컬럼 순서를 조정한다.df = df.rename(columns={'날짜':'date','종가':'close','전일비':'diff'                ,'시가':'open','고가':'high','저가':'low','거래량':'volume'}) #영문으로 컬럼명 변경df['date'] = pd.to_datetime(df['date']) df = df.dropna() # 결측치 제거df[['close', 'diff', 'open', 'high', 'low', 'volume']] = df[['close','diff', 'open', 'high', 'low', 'volume']].astype(int) # BIGINT형으로 지정한 컬럼을 int형으로 변경df = df[['date', 'open', 'high', 'low', 'close', 'diff', 'volume']]df = df.sort_values(by = 'date') # 날짜순으로 정렬df.head()                  date      open      high      low      close      diff      volume                  13      2021-08-02      79200      79500      78700      79300      800      11739124              12      2021-08-03      79400      81400      79300      81400      2100      24339360              11      2021-08-04      82200      83100      81800      82900      1500      25642368              10      2021-08-05      83300      83300      82000      82100      800      18485469              9      2021-08-06      81900      82500      81300      81500      600      13342623      3. 함수로 정리하기def get_krx_code(market=None):    market_type = ''    if market == 'kospi':        market_type = '&amp;marketType=stockMkt'    elif market == 'kosdaq':        market_type = '&amp;marketType=kosdaqMkt'    elif market == 'konex':        market_type = '&amp;marketType=konexMkt'            url = 'http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&amp;searchType=13{0}'.format(market_type)    stock_code = pd.read_html(url, header = 0)[0]    stock_code['종목코드'] = stock_code['종목코드'].map('{:06d}'.format)    stock_code = stock_code[['회사명', '종목코드', '업종', '상장일']]    stock_code = stock_code.rename(columns = {'회사명': 'name', '종목코드': 'code', '업종': 'sectors',                                              '상장일': 'listing_date'})    stock_code['listing_date'] = pd.to_datetime(stock_code['listing_date'])        return stock_codedef get_stock_price(code, num_of_pages, sort_date = True):    url = f\"http://finance.naver.com/item/sise_day.nhn?code={code}\"    headers = {'User-agent': 'Mozilla/5.0'}     bs = BeautifulSoup(requests.get(url=url, headers = headers).text, 'html.parser')    pgrr = bs.find(\"td\", class_=\"pgRR\")    last_page = int(pgrr.a[\"href\"].split('=')[-1])        pages = min(last_page, num_of_pages) # 마지막 페이지와 가져올 페이지 수 중에 작은 값 선택    df = pd.DataFrame()    for page in range(1, pages+1):        page_url = '{}&amp;page={}'.format(url, page)        df = df.append(pd.read_html(requests.get(page_url, headers={'User-agent': 'Mozilla/5.0'}).text)[0])            df = df.rename(columns={'날짜':'date','종가':'close','전일비':'diff'                ,'시가':'open','고가':'high','저가':'low','거래량':'volume'}) #영문으로 컬럼명 변경    df['date'] = pd.to_datetime(df['date'])     df = df.dropna()    df[['close', 'diff', 'open', 'high', 'low', 'volume']] = \\                            df[['close','diff', 'open', 'high', 'low', 'volume']].astype(int) # int형으로 변경    df = df[['date', 'open', 'high', 'low', 'close', 'diff', 'volume']] # 컬럼 순서 정렬    df = df.sort_values(by = 'date') # 날짜순으로 정렬        if sort_date:        df = df.reset_index(drop = True)        return dfitem_name = '삼성전자'stock = get_krx_code().query(\"name=='{}'\".format(item_name))['code'].to_string(index=False) df = get_stock_price(stock, 10)df.head()                  date      open      high      low      close      diff      volume                  0      2021-08-02      79200      79500      78700      79300      800      11739124              1      2021-08-03      79400      81400      79300      81400      2100      24339360              2      2021-08-04      82200      83100      81800      82900      1500      25642368              3      2021-08-05      83300      83300      82000      82100      800      18485469              4      2021-08-06      81900      82500      81300      81500      600      13342623      ",
        "url": "/trading03"
    }
    ,
    
    "it03": {
        "title": "git cheat sheet3",
            "author": "km.yu99",
            "category": "",
            "content": "이 포스트는 여러 절로 구성되어 있습니다.   cheat sheet1 - 기본 명령어  cheat sheet2 - commit 조작  cheat sheet3 - branch 조작git에 대한 자세한 내용은 다음 책을 참고 바란다.본 절에서는 브랜치 작업과 관련된 명령어를 정리하였다.1. Setup  git branch : 브랜치 목록 표시  git branch [브랜치명] : 해당 브랜치 명으로 브랜치 생성  git checkout [브랜치명] : 해당 브랜치로 전환  git checkout –b [브랜치명] : 브랜치 생성과 동시에 전환  git branch -m [브랜치명] [새로운 브랜치명] : 브랜치명 변경  git branch –d [브랜치명] : 해당 브랜치 삭제2. Merge, rewrite2.1 merge  git merge [브랜치명] : 현 브랜치에 해당 브랜치의 내용 병합  git merge --ff [브랜치명] : fast-forward 관계에 있으면 commit을 생성하지 않고 현재 브랜치의 참조 값 만 변경(default)  git merge --no-ff [브랜치명] : fast-forward 관계에 있어도 merged commit 생성  git merge --squash [브랜치명] : fast-forward 관계에 있어도 merged commit 생성, merging 브랜치 정보 생략2.2 rebase  git rebase [브랜치명] : 현재 브랜치가 해당 브랜치(브랜치명)에부터 분기하도록 재배치  git rebase --continue : 충돌 수정 후 재배치 진행(commit 대신)  git rebase --abort : rebase 취소2.3 cherry-pick  git cherry-pick [commit hash] : 해당 commit의 내용을 현재 브랜치에 추가. 뒤에 commit hash 를 연속 입력하면 복수 추가 가능  git cherry-pick [commit hash start].. [commit hash end] : 해당 구간의 commit을 한번에 추가  git cherry-pick –-abort :  충돌과 같은 상황이 발생했을 때 cherry-pick 취소  git cherry-pick –-continue : 충돌 상황 해결 후 cherry-pick 진행  git cherry-pick –m [parent number] [merge commit ID] : merge commit을 추가. merge commit의 경우 어떤 부분의 merge를 가져올지 알 수 없다. 그래서 parent number를 추가해야 한다.(1부터 시작하며 main line이 1)",
        "url": "/it03"
    }
    ,
    
    "it02": {
        "title": "git cheat sheet2",
            "author": "km.yu99",
            "category": "",
            "content": "이 포스트는 여러 절로 구성되어 있습니다.   cheat sheet1 - 기본 명령어  cheat sheet2 - commit 조작  cheat sheet3 - branch 조작git에 대한 자세한 내용은 다음 책을 참고 바란다.본 포스트에서는 commit 조작과 관련된 명령어를 정리하였다.1. Checkout  git checkout [commit hash] : 해당 commit으로 파일상태 변경  git checkout - : HEAD가 이전에 참조했던 commit으로 상태변경  git checkout master : HEAD가 master를 참조  git checkout HEAD~n : HEAD를 기준으로 n단계 이전 commit으로 상태변경2. Undoing checkout  git reset : Staging area의 파일 전체를 unstaged 상태로 되돌리기  git reset [파일명] : 해당 파일을 unstaged 상태로 되돌리기  git commit --amend : 최근 커밋을 수정하기  git commit --amend -m \"[commit 메시지]\" : 해당 메시지로 commit 수정하기  git reset [commit hash] : 해당 commit으로 브랜치의 참조를 변경  git reset –-hard [commit hash] : working directory, staging area, commit 모두 reset  git reset –-mixed [commit hash] : working directory 유지, staging area, commit reset , default option  git reset –-soft [commit hash] : working directory, staging area 유지, commit reset  git reset HEAD^ : HEAD를 기준으로 직전의 commit으로 reset  git reset HEAD~[정수] : HEAD를 기준으로 정수 값 단계 전 commit으로 reset",
        "url": "/it02"
    }
    ,
    
    "it01": {
        "title": "git cheat sheet1",
            "author": "km.yu99",
            "category": "",
            "content": "이 포스트는 여러 절로 구성되어 있습니다.   cheat sheet1 - 기본 명령어  cheat sheet2 - commit 조작  cheat sheet3 - branch 조작git에 대한 자세한 내용은 다음 책을 참고 바란다.본 포스트에서는 git 기본 명령어와 옵션 별 기능을 정리하였다.1. Setup  git init : 저장소(repository) 생성  git clone [원격 저장소 url] : 해당 주소의 내용을 복제하여 저장소 생성  git config user.name [작성자 이름] : 작성자 이름 설정  git config user.email [이메일 계정] : 작성자 이메일 설정  git config --list : 저장소 설정 전체 출력  git config --get [설정항목] : 일부 설정항목만 출력(ex : git config –get user.name)  git help [커맨드 이름] : 도움말2. Stage &amp; commit  git add [파일이름] : 수정된 파일을 staging area 올리기  git add [디렉토리 명] : 해당 디렉토리 내에 수정된 모든 파일들을 staging area에 올리기  git add . : working directory 내에 수정된 모든 파일들을 staging area에 올리기 (untracked 파일 제외)  git commit : 이력 저장(commit)  git commit -m \"[메시지]\" : vim을 사용하지 않고 인라인으로 메시지를 추가하여 commit  git commit -am \"[메시지]\" : add와 commit을 일괄적으로 진행3. Inspectgit status  git status : 저장소 파일의 상태정보 출력  git status -s : 파일 상태정보를 간략하게 표시git log  git log : 저장소의 commit이력을 출력  git log --pretty=oneline : 각 commit을 한줄로 출력(–pretty 옵션 사용)  git log --oneline : 각 commit을 한줄로 출력  git log --decorate=full : 브랜치나 태그정보를 상세히 출력  git log --graph : 그래프 형태로 출력git show  git show : 가장 최근의 commit 정보 출력  git show [commit hash] : 해당 commit의 정보 출력  git show HEAD : HEAD가 참조하는 commit의 정보 출력  git show HEAD^^^ : HEAD를 기준으로 3단계 이전의 commit정보 출력  git show HEAD~[n] : HEAD를 기준으로 n단계 이전의 commit정보 출력git diff  git diff : 최근 commit과 변경사항이 발생한(Unstaged) 파일들의 내용비교  git diff --staged : 최근 commit과 Staging area의 파일들 간의 변경사항 출력  git diff [commit hash1] [commit hash2] : 두 commit의 파일들 간의 변경사항 출력",
        "url": "/it01"
    }
    ,
    
    "ds02": {
        "title": "custom dataset으로 YOLOv5 학습하기2",
            "author": "km.yu99",
            "category": "",
            "content": "이 포스트는 여러 절로 구성되어 있습니다.     YOLO5 #1 - custom dataset으로 학습하기    YOLO5 #2 - 학습 파라미터 설정하기이전 포스트에서 custom dataset으로 YOLOv5 모델을 학습시키는 방법에 대해서 설명하였다. 여기서는 학습과 관련된 파라미터를 조정하는 방법에 대해서 설명한다. 앞에서와 마찬가지로 실습환경은 google colab이다.1. 데이터셋 소개실습에 사용되는 데이터셋은 roboflow에서 제공되는 Mask Wearking Dataset(raw)이다.[링크](raw와 416x416으로 변환된 데이터셋을 선택할 수 있는데, 여기서는 raw 데이터셋을 사용한다.)2. colab에서 환경구축하기환경구축은 앞과 같기 때문에 간력하게 설명한다. 상세한 내용은 이전 포스트를 참고한다. [링크]  google colab에 접속하고 새 노트를 생성      런타임-런타임 유형 변경을 선택후, 가속기를 GPU로 설정    yolov5 파일을 다운로드 및 필수 라이브러리를 설치!git clone https://github.com/ultralytics/yolov5  # yolov5 코드 clone%cd yolov5 \t\t\t\t\t\t\t\t\t\t  # clone한 폴더로 진입%pip install -qr requirements.txt                 # 필수 라이브러리 설치      custom dataset 업로드 (여기서는 mask_dataset.zip 으로 설명)        데이터 셋 파일 압축 해제  !unzip ../custom_dataset.zip  ` yolov5/data/ 폴더에 mask_dataset.yaml`파일 작성path: /content/yolov5/mask_datasettrain: train/imagesval: valid/imagestest: test/imagesnc: 2names: ['mask', 'no-mask']3. 학습 파라미터 살펴보기이제 학습할 때 파라미터를 조정하는 방법에 대해서 설명한다.모델을 학습할 때 다음과 같이 데이터셋 관련 경로만 입력하면 가능하다.!python train.py --data \"데이터셋.yaml 파일 경로\"나머지 파라미터들은 디펄트 값으로 대체된다. 그러면 학습과 관련된 파라미터에는 어떤 것들이 있을까? yolov5 폴더 안에 있는 train.py 파일을 열어서 440번째 라인 부근에 있는 parse_opt 함수를 살펴보자. 아래와 같이 파라미터들이 정의되어 있다.def parse_opt(known=False):    parser = argparse.ArgumentParser()    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch.yaml', help='hyperparameters path')    parser.add_argument('--epochs', type=int, default=300)    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')    parser.add_argument('--rect', action='store_true', help='rectangular training')        --- (생 략)학습과 관련된 파라미터가 40여개나 된다. 하지만 모두 알 필요는 없다. 모델 성능이나 하드웨어 자원과 관련된 중요한 파라미터만 살펴보자.  모델구조 (–weights)모델 구조와 관련된 파라미터이다. YOLOv5는 다양한 모델 구조를 제공한다. default 값은 YOLOv5로 구조가 제일 간단하다. 모델의 구조가 더 복잡한 것으로 YOLOv5m, YOLOv5l, YOLOv5x 이 있다.구조가 복잡할 수록 성능이 높아질 가능성은 높지만 학습할 때 더 많은 시간이 소요되고 많은 리소스가 요구된다. 예를 들어 yolov5m 모델을 학습시키고 싶다면 다음과 같이 입력하면 된다.--weights \"yolov5m.pt\"공식 github에는 이외에 새로운 모델 구조가 지속적으로 업로드되고 있다. [링크]  배치 사이즈 (–batch-size)학습할 때 한번에 처리할 이미지 수(batch-size)를 지정할 수 있다. default는 16이다. batch size를 32로 입력하고 싶다면 다음과 같이 옵션 설정을 하면된다.--batch-size 32  이미지 크기 (–imgsz,  –img, –img-size)YOLOv5는 학습할 때 모든 이미지 사이즈를 동일하게 resizing 한다. default 사이즈는 640x640이다. 이미지 사이즈를 크게 설정할수록 모델 성능은 더 좋아실 수 있다. 하지만 학습속도와 리소스 부담은 더 커지게 된다. 이미지 크기를 1280x1280으로 설정하고 싶다면 다음과 같이 입력한다.--imgsz(or --img or --img-size) 1280검증이나 시험할 때 학습에 사용한 이미지 사이즈와 동일하게 설정해야한다.  에포크 수 (–epochs)데이터셋으로 학습을 반복할 횟수를 지정하는 에포크의 default 값은 300이다. 100으로 설정하고 싶다면 다음과 같이 입력한다.--epochs 100  하이퍼 파라미터 (–hyp)하이퍼 파라미터가 정의되어 있는 경로를 지정한다. default 값은 data/hyps/hyp.scratch.yaml이다. 해당 경로의 파일을 열어 확인해보자.4. 파라미터를 조정하여 모델 학습하기colab에서 제공하는 자원을 최대한 사용하여 학습을 진행해보자. 모델 구조는 yolov5m.pt, 입력 이미지 크기는 1280, 배치 사이즈는 8, 에포크 수는 60으로 설정해보자. (모델 구조가 커지고 입력 이미지가 복잡해져서 colab gpu 한계를 맞추기 위해 배치 사이즈와 학습시간을 줄여야만 했다.)!python train.py --data \"data/mask_dataset.yaml\" --batch-size 8 --img 1280 --weights \"yolov5m.pt\" --epochs 60train: weights=yolov5m.pt, cfg=, data=data/mask_dataset.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=60, batch_size=8, imgsz=1280, rect=False, --(생 략)hyperparameters: lr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, --(생 략)                 from  n    params  module                                  arguments                       0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]                1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                  2                -1  2     65280  models.common.C3                        [96, 96, 2]   --(생 략)Model Summary: 290 layers, 20856975 parameters, 0 gradients, 48.0 GFLOPs               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:06&lt;00:00,  3.12s/it]                 all         29        162       0.81      0.853      0.861      0.531                mask         29        142      0.871      0.805      0.888      0.559             no-mask         29         20       0.75        0.9      0.835      0.503Results saved to runs/train/exp3여기서는 학습 결과가 /runs/train/exp3에 저장되었다. 사용자 마다 저장위치가 다를 것이다.5. 검증하기검증과 관련된 파라미터는 val.py 파일의 306번째 라인의 parse_opt 함수에 정의되어 있다.학습할 때 이미지 사이즈는 1280으로 설정하였고, 모델 가중치는 /runs/train/exp3/weights/best.pt 저장되어 있으므로 다음과 같이 입력하여 검증을 진행하자.!python val.py --data \"data/mask_dataset.yaml\" --img 1280 --weights \"/content/yolov5/runs/train/exp3/weights/best.pt\"val: data=data/mask_dataset.yaml, weights=['/content/yolov5/runs/train/exp3/weights/best.pt'], batch_size=32, imgsz=1280, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=FalseYOLOv5 🚀 v6.0-159-gdb6ec66 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)Fusing layers... Model Summary: 290 layers, 20856975 parameters, 0 gradients, 48.0 GFLOPsval: Scanning '/content/yolov5/mask_dataset/valid/labels.cache' images and labels... 29 found, 0 missing, 0 empty, 0 corrupted: 100% 29/29 [00:00&lt;?, ?it/s]               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:08&lt;00:00,  8.05s/it]                 all         29        162      0.813      0.851      0.862      0.535                mask         29        142      0.878       0.81      0.887      0.556             no-mask         29         20      0.748      0.892      0.837      0.514Speed: 1.5ms pre-process, 166.8ms inference, 4.4ms NMS per image at shape (32, 3, 1280, 1280)Results saved to runs/val/exp예측결과가 runs/val/exp에 저장되었다.6. 예측하기예측할 때는 기본적으로 모델의 경로(–weights), 입력 데이터 경로(–source)를 지정해줘야 한다. 여기에 추가로 이미지 사이즈(–img)도 지정해주자.!python detect.py --img 1280 --weights \"/content/yolov5/runs/train/exp3/weights/best.pt\" --source \"/content/yolov5/mask_dataset/test/images\"detect: weights=['/content/yolov5/runs/train/exp3/weights/best.pt'], source=/content/yolov5/mask_dataset/test/images, imgsz=[1280, 1280], conf_thres=0.25, ---(생 략)Results saved to runs/detect/expruns/detect/exp에 예측 결과가 저장된다.참 고 : 기회가 되면 confidence threshold(--conf-thres)와 IoU threshold(--iou-thres) 도 변경해가며 예측결과를 비교해보자.",
        "url": "/ds02"
    }
    ,
    
    "ds01": {
        "title": "custom dataset으로 YOLOv5 학습하기1",
            "author": "km.yu99",
            "category": "",
            "content": "이 포스트는 여러 절로 구성되어 있습니다.     YOLO5 #1 - custom dataset으로 학습하기    YOLO5 #2 - 학습 파라미터 설정하기YOLO(You Only Look Once)는 널리 쓰이는 object detection 알고리즘이다. 최근에는 YOLOv5 까지 출시되었다. 여기서는 공식 github 계정에 업로드된 YOLOv5 코드로 custom dataset을 학습하는 방법에 대하여 설명한다. google colab 환경에서 진행되었다.1. 데이터셋 소개실습에 사용되는 데이터셋은 roboflow에서 제공되는 North American Mushrooms Dataset이다.[링크]여기서는 학습시간을 줄이기 위해서 416x416 사이즈의 이미지 51장을 다운 받았다.object detection 알고리즘 라이브러리 구현방식에 따라, 그리고 YOLO 버전 별로도 사용하는 레이블링 파일의 포맷이 다르다. roboflow에서는 레이블링 파일 포맷을 선택하여 다운도르 할 수 있다. 우리는 PyTorch로 구현된 공식 계정의 코드를 사용할 예정이므로 YOLO v5 PyTorch를 선택하고 다운로드 한다.참고로 YOLOv5 공식계정의 코드는 txt 포맷의 레이블링 데이터를 사용한다.이 파일은 이미지에서 검출된 object에 대한 클래스와 bounding box 정보를 포함하고 있다. 검출 객체정보 배치는 [class, x_center, y_center, width, height] 형태로 되어있다. bounding box 정보는 이미지 사이즈에 의해 정규화 되어있다. 따라서 0~1 범위의 값을 가진다.편의를 위해 다운로드한 데이터 셋 압축파일의 폴더 이름을 custom_dataset으로 수정한다. 데이터 폴더 구성은 다음과 같다.custom_dataset│├── test/│   ├── images/                    │   └── labels/             │├── train/│   ├── images/                │   └── labels/               │├── valid/│   ├── images/                    │   └── labels/   │├── data.yaml└── README.dataset.txtdata.yaml파일을 메모장으로 열어보자. 데이터 셋 기본정보가 포함되어 있다. 우리는 *.yaml 파일을 새로 만들 것이다. 어떤 식으로 구성되는지 참고만 한다.train과 val은 각 데이터 셋의 경로정보이다. 그리고 nc는 class의 수(number of classes)를, names는 각 클래스의 이름이다.2. colab에서 환경구축하기google colab에 접속하고 새 노트를 생성한다. 런타임-런타임 유형 변경을 선택하여, 하드웨어 가속기를 GPU로 설정한다.이제 colab 노트에 공식 github 계정의 파일을 다운로드하고, 필수 라이브러리를 설치하는 명령을 입력한다.!git clone https://github.com/ultralytics/yolov5  # yolov5 코드 clone%cd yolov5 \t\t\t\t\t\t\t\t\t\t  # clone한 폴더로 진입%pip install -qr requirements.txt                 # 필수 라이브러리 설치Cloning into 'yolov5'...remote: Enumerating objects: 10354, done.remote: Total 10354 (delta 0), reused 0 (delta 0), pack-reused 10354Receiving objects: 100% (10354/10354), 10.58 MiB | 23.75 MiB/s, done.Resolving deltas: 100% (7149/7149), done./content/yolov5     |████████████████████████████████| 596 kB 5.4 MB/s 파일 탐색기에 yolov5 폴더가 생성되었고, 파일들이 다운로드 되어있다.이제 앞에서 다운로드한 데이터셋을 업로드 한다. 파일 탐색기의 업로드 아이콘을 클릭하여 custom_dataset.zip 파일을 업로드 한다.업로드가 완료되면 탐색기에 해당 파일이 표시된다.unzip 명령으로 데이터 셋 파일의 압축을 해제한다.!unzip ../custom_dataset.zipArchive:  ../custom_dataset.zip  inflating: custom_dataset/data.yaml    inflating: custom_dataset/README.dataset.txt    inflating: custom_dataset/README.roboflow.txt     creating: custom_dataset/test/   creating: custom_dataset/test/images/  inflating: custom_dataset/test/images/chanterelle_02_jpg.rf.f7a48494b7393c532f641585d99a57be.jpg    inflating: custom_dataset/test/images/chanterelle_03_jpg.rf.580f8d787af6a8050c21c065bf016f20.jpg    inflating: custom_dataset/test/images/chanterelle_03_jpg.rf.cd892d2f06d228ba20d194fc360320fc.jpg    --- (생 략)완료되면 yolov5/custom_dataset/ 경로에 데이터 셋이 위치하게 된다. (현 작업 디렉토리가 yolov5이기 때문)마지막으로 데이터 셋 설정파일을 작성한다.` yolov5/data/ 폴더에 custom_dataset.yaml`이라는 이름의 파일을 생성한다.여기에 다음과 같이 설정정보를 입력한다.path: /content/yolov5/custom_dataset  #root 디렉토리train: train/images\t\t\t\t\t  # 학습데이터 경로val: valid/imagestest: test/imagesnc: 2\t\t\t\t\t\t\t\t# 클래스 수names: ['CoW', 'chanterelle']\t\t# 클래스 이름자세한 내용은 다음 링크의 **1.1 Create dataset.yaml** 항목을 참고하자.이로써 학습을 위한 모든 준비가 완료 되었다.3. 모델 학습하기모델 학습 순서는 다음과 같다.위와 같은 일련의 과정은 train.py  파일 실행을 통해 가능하다. 인자로 학습 데이터 경로와 epoch 수를 입력하고 학습을 진행하자.!python train.py --data \"data/custom_dataset.yaml\" --epochs 100 #epoch 100회 Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf... train: weights=yolov5s.pt, cfg=, data=data/custom_dataset.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=100, batch_size=16, imgsz=640, '''  ---(생략)  Overriding model.yaml nc=80 with nc=2                   from  n    params  module                                  arguments                        0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]                 1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                   2                -1  1     18816  models.common.C3                        [64, 64, 1]    ---(생략)  Logging results to runs/train/exp Starting training for 100 epochs...       Epoch   gpu_mem       box       obj       cls    labels  img_size       0/99     3.23G    0.1252   0.03226   0.02699        28       640: 100% 3/3 [00:05&lt;00:00,  1.95s/it]                Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00&lt;00:00,  2.48it/s]                  all          5         14    0.00695      0.311    0.00395     0.0011        ---(생략)  Validating runs/train/exp/weights/best.pt... Fusing layers...  Model Summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs                Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00&lt;00:00,  4.48it/s]                  all          5         14       0.95      0.996      0.973      0.697                  CoW          5          5          1      0.991      0.995      0.688          chanterelle          5          9      0.899          1      0.951      0.706 Results saved to runs/train/exp학습이 완료되면 runs/train/exp경로에 학습 결과가 저장된다. 학습을 반복하면 runs/train경로에 exp1, 2, 3… 같은 형태로 폴더가 생성되면서 학습 결과가 기록된다.학습 결과를 다운로드 하고 싶다면 zip 명령을 압축한 뒤, 저장한다 . 예를 들어 train_result.zip이라는 이름으로 압축하고 싶다면 다음과 같이 입력한다.!zip -r train_result.zip /content/yolov5/runs/train/exp탐색기에 train_result.zip가 표시되면 정상으로 압축된 것이다.4. 학습한 모델 검증하기이제 학습한 모델로 검증을 진행해보자. 검증순서는 앞의 학습 절차에서 모델 가중치 업데이트 과정이 생략된 것이다.모델 검증은 val.py  파일 실행을 통해 진행한다. 다양한 인자가 있지만 데이터 경로(--data), 모델 가중치(--weights) 정도만 입력해서 실행해보자. 앞에서 학습한 모델 가중치는 runs/train/exp/weights/best.pt에 저장되었다.!python val.py --data \"data/custom_dataset.yaml\" --weights \"/content/yolov5/runs/train/exp/weights/best.pt\"val: data=data/custom_dataset.yaml, weights=['/content/yolov5/runs/train/exp/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=FalseYOLOv5 🚀 v6.0-155-gdc54ed5 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)Fusing layers... Model Summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPsval: Scanning '/content/yolov5/custom_dataset/valid/labels.cache' images and labels... 5 found, 0 missing, 0 empty, 0 corrupted: 100% 5/5 [00:00&lt;?, ?it/s]               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00&lt;00:00,  2.58it/s]                 all          5         14      0.909      0.982      0.961      0.686                 CoW          5          5          1      0.965      0.995      0.672         chanterelle          5          9      0.818          1      0.926        0.7Speed: 0.6ms pre-process, 29.3ms inference, 3.0ms NMS per image at shape (32, 3, 640, 640)Results saved to runs/val/exp검증결과는 runs/val/exp에 저장된다. 앞에서와 마찬가지로 다운로드 받고 싶다면 폴더를 압축하자.!zip -r val_result.zip /content/yolov5/runs/valexp 폴더 안에는 confusion matrix, F1 curve 등 성능과 관련된 차트가 저장되어 있다.5. 학습한 모델로 예측하기예측과정은 아래 그림과 같은 절차로 진행된다.예측 과정은 detect.py 파일을 사용한다. 단순 이미지 뿐만 아니라 웹캠, 비디오 파일 등에서도 실행 가능하다. --source인자에 다음과 같이 설정해주면 된다.!python detect.py --source 0  # webcam                            img.jpg  # image                            vid.mp4  # video                            path/  # directory                            path/*.jpg  # glob                            'https://youtu.be/Zgi9g1ksQHc'  # YouTube                            'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream여기서는 custom_dataset/test/images 경로에 있는 이미지에 대해서 object detection을 실행해본다. 인식 대상 (--source), 모델 가중치(--weights)  경로를 입력해서 실행해보자.!python detect.py --weights \"/content/yolov5/runs/train/exp/weights/best.pt\" --source \"/content/yolov5/custom_dataset/test/images\"detect: weights=['/content/yolov5/runs/train/exp/weights/best.pt'], source=/content/yolov5/custom_dataset/test/images, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, ---(생략)Fusing layers... Model Summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPsimage 1/5 /content/yolov5/custom_dataset/test/images/chanterelle_02_jpg.rf.f7a48494b7393c532f641585d99a57be.jpg: 640x640 3 chanterelles, Done. (0.034s)--- (생략)테스트 결과는 /runs/detect/exp 경로에 저장된다. 결과를 다운로드 하고 싶다면 다음과 같이 압축하여 저장한다.!zip -r test_result.zip /content/yolov5/runs/detect/exp폴더를 열어보면 class와 bounding box가 표시된 detection 결과 이미지가 저장되어 있다.",
        "url": "/ds01"
    }
    ,
    
    "trading2": {
        "title": "KRX에서 종목코드 가져오기2",
            "author": "km.yu99",
            "category": "",
            "content": "한국거래소(KRX)에서 증권과 관련된 데이터와 통계정보를 제공하는 정보데이터시스템 웹페이지가 있다.여기서도 상장주식의 종목코드 리스트를 가져올 수 있다.1. 홈페이지 둘러보기KRX의 전자정보데이터시스템에 접속한다.  http://data.krx.co.kr/contents/MDC/MAIN/main/index.cmd‘주식’-‘종목정보’-‘전종목 지정내역’ 또는 [다음] 링크를 클릭한다.해당 화면에서 국내 상장주식의 기본정보를 조회할 수 있다. 우측에 다운로드 아이콘을 클릭하면 원하는 포맷으로 데이터를 다운로드 할수도 있다.크롬에서 ‘F12’버튼 또는 마우스 우클릭 후 ‘검사’를 클릭하여 개발자 도구 창을 연다.상단 탭에서 ‘Network’를 선택한다. 이 상태에서 앞에서 설명한 다운로드 항목에서 ‘CSV’ 아이콘을 클릭하여 파일 다운로드를 실행해본다.개발자 도구창 하단에 ‘generate.cmd’, ‘download.cmd’가 새로 생성되었다.즉, 파일 다운로드 과정은 두단계를 거치게 된다. ‘generate.cmd’에 의해서 데이터를 다운로드 하기 위한 code를 리턴한다. 이 후 ‘download.cmd’로 code를 파라미터로 하여 데이터를 저장한다.‘generate.cmd’를 선택하고 상단의 ‘Headers’를 클릭해보자.요청할 주소와 방식(POST) 등을 확인할 수 있다.‘Payload’를 클릭하면 요청할 때 사용된 파라미터를 확인된다.마찬가지로 ‘download.cmd’를 선택하고 상단의 ‘Headers’를 클릭해보자.요청할 주소와 방식(POST)을 확인할 수 있다.‘Payload’에는 code 값이 표시되어 있다. 이 값은 ‘generate.cmd’로 받아와야 하는 값이다.이 정보를 바탕으로 코드를 작성한다.2. 추출하기웹에서 정보를 가져오기 위해 requests 라이브러리를 사용할 것이다.필요한 라이브러리를 import 한다.import requestsimport pandas as pdfrom io import BytesIO첫번째 단계로 generate 요청을 진행한다.요청할 주소와 파라미터를 설정하자. 앞에 소개한 ‘generate.cmd’에 명시된 내용이다.gen_url = 'http://data.krx.co.kr/comm/fileDn/GenerateOTP/generate.cmd'gen_parms = {    'mktId': 'ALL',    'share': '1',    'csvxls_isNo': 'false',    'name': 'fileDown',    'url': 'dbms/MDC/STAT/standard/MDCSTAT01901'    }python 코드에 의한 접근이 아닌 크롬으로 접근하고 있음을 표시하기 위해 header 정보를 추가한다. ‘Headers’ 하단에 있는 ‘Request Headers’ 항목을 참고하여 작성한다.headers = {    'Referer': 'http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0201020101',    'Upgrade-Insecure-Requests': '1',    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36' #generate.cmd에서 찾아서 입력하세요    }code 값을 리턴받기 위해 요청을 진행한다.r = requests.get(url=gen_url, params=gen_parms, headers=headers)r.contentb'a1n6kaOi+6ccSQWhSJQn6cmnCCnJIeb940e8ATaeiE4RtSksuLS7Bnxpl86F7dAOvXfGx9S2U5wgvoxsacATRRtmGtORI4WrGDmruVe6oXtCqUypoW0Lp6SAPP0PhVkgThCTcjIZNPI5lCTubZnhjio6AHXdxc45YVEhz4JdugHPMxvIwHadpQpCGE1HxZAXvTCprTIXuXT9XxFb88awpQ=='r.content에 code 값이 저장되었다.이제 두번째 단계, 다운로드 요청을 진행한다.‘download.cmd’에 표시된 요청주소를 참고하고, code는 앞에서 받은 값을 지정한다.down_url = 'http://data.krx.co.kr/comm/fileDn/download_csv/download.cmd'data = {    'code': r.content}r = requests.post(url=down_url, data=data, headers=headers)r.content이렇게 받은 정보는 바이너리 데이터이다.이 데이터를 다루기 위해 BytesIO 메서드를 사용한다.이제 pandas의 read_csv 메서드로 데이터를 읽어온다.stock_code = pd.read_csv(BytesIO(r.content), encoding='cp949')stock_code.head()필요한 컬럼만 가져오고 컬럼명을 영문으로 변경한다.stock_code = stock_code[['한글 종목약명', '단축코드', '시장구분', '액면가', '상장주식수']]stock_code = stock_code.rename(columns = {'시장구분': 'market', '한글 종목약명': 'name', '단축코드': 'code',                                           '액면가': 'par_value', '상장주식수': 'total_shrs'})stock_code.head()                  name      code      market      par_value      total_shrs                  0      마이크로컨텍솔      098120      KOSDAQ      500      8312766              1      스카이이앤엠      131100      KOSDAQ      500      11642629              2      포스코엠텍      009520      KOSDAQ      500      41642703              3      AJ네트웍스      095570      KOSPI      1000      46822295              4      AK홀딩스      006840      KOSPI      5000      13247561      3. 함수로 정리하기def get_krx_code():    gen_url = 'http://data.krx.co.kr/comm/fileDn/GenerateOTP/generate.cmd'    gen_parms = {        'mktId': 'ALL',        'share': '1',        'csvxls_isNo': 'false',        'name': 'fileDown',        'url': 'dbms/MDC/STAT/standard/MDCSTAT01901'        }    headers = {    'Referer': 'http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0201020101',    'Upgrade-Insecure-Requests': '1',    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36' #generate.cmd에서 찾아서 입력하세요    }    r = requests.get(url=gen_url, params=gen_parms, headers=headers)        down_url = 'http://data.krx.co.kr/comm/fileDn/download_csv/download.cmd'    data = {        'code': r.content    }    r = requests.post(url=down_url, data=data, headers=headers)        stock_code = pd.read_csv(BytesIO(r.content), encoding='cp949')    stock_code = stock_code[['한글 종목약명', '단축코드', '시장구분', '액면가', '상장주식수']]    stock_code = stock_code.rename(columns = {'시장구분': 'market', '한글 종목약명': 'name', '단축코드': 'code',                                               '액면가': 'par_value', '상장주식수': 'total_shrs'})        return stock_code",
        "url": "/trading2"
    }
    ,
    
    "trading1": {
        "title": "KRX에서 종목코드 가져오기1",
            "author": "km.yu99",
            "category": "",
            "content": "1. 홈페이지 둘러보기먼저 KRX 홈페이지(https://kind.krx.co.kr/) 를 방문하여 ‘상장법인상세정보’/ ‘상장법인목록’ 화면으로 이동한다.(또는 [다음] 링크를 클릭한다.)‘EXECEL’ 버튼을 클릭하면 전체 상장법인 목록을 다운받을 수 있다.다운로드한 파일은 ‘xls’ 확장자를 가지고 있지만 html 포맷이다. 우리는 파이썬 코드로 위 목록을 가져와 pandas 데이터프레임으로 변환하고자 한다.다시 돌아와서 상장법인 목록을 다운로드 하는 방식을 살펴보자.‘시장구분’, ‘검색유형’ 등과 같은 범주를 설정하고 ‘EXCEL’ 버튼을 클릭하면 해당 범주에 해당하는 법인목록이 다운로드 되는 방식이다.설정항목을 수정하지 않고 EXCEL 버튼을 클릭하면 전체 상장법인 목록이 다운로드 된다.크롬에서 해당 페이지를 열고 ‘상장법인목록’ 부분을 마우스 우클릭하고, ‘검사’ 항목을 선택하자.검색 항목의 소스를 확인할 수 있다.요약하면 ‘searchForm’이라고 하는 양식을 통해서 추출 조건을 지정한다. 그리고 ‘post’를 사용해서 데이터 추출을 요청한다.추출 처리를 담당하는 서버의 주소는 ‘http://kind.krx.co.kr/corpgeneral/corpList.do’ 이다.그리고 ‘EXCEL’ 버튼을 클릭했을 때, 동작하는 메커니즘을 살펴보자.버튼을 클릭하면 ‘fnDownolad()’ 함수를 호출한다.이 함수는 searchForm에 설정된 조건대로 데이터를 다운로드 하도록 서버 프로그램에 요청하는 함수이다.2. 추출하기다운되는 종목코드 파일이 html 형식이므로 pandas의 read_html 함수를 사용한다.read_html의 인자에 다운로드를 요청할 url을 지정하면 된다.서버의 url 뒤에 ‘?’로 url의 끝을 표시하고 데이터(검색조건)를 추가하면 된다.주요 조건은 다음과 같다.  method: 메서드, 예시) ‘download’  orderMode:정렬컬럼, 예시) ‘1’,  orderStat: 정렬 내림차순, 예시) ‘D’  searchType: 검색유형, 예시) ‘13’(상장법인)  fiscalYearEnd: 결산원, 예시) ‘all’(전체)  location: 지역, 예시) ‘all’(전체)여기서는 다음과 같이 설정한다. (나머지는 default 값 적용)url = 'http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&amp;searchType=13'import pandas as pdimport numpy as npimport osstock_code = pd.read_html(url, header = 0)[0] # 주의 : [0]을 반드시 추가. read_html은 table들을 읽어 리스트로 저장함stock_code.head()                  회사명      종목코드      업종      주요제품      상장일      결산월      대표자명      홈페이지      지역                  0      DL      210      기타 금융업      지주회사      1976-02-02      12월      전병욱      http://www.dlholdings.co.kr      서울특별시              1      DRB동일      4840      고무제품 제조업      고무벨트(V벨트,콘베이어벨트,평벨트),프라스틱제품 제조,판매      1976-05-21      12월      류영식      http://drbworld.com      부산광역시              2      DSR      155660      1차 비철금속 제조업      합섬섬유로프      2013-05-15      12월      홍석빈      http://www.dsr.com      부산광역시              3      GS      78930      기타 금융업      지주회사/부동산 임대      2004-08-05      12월      허태수, 홍순기 (각자 대표이사)      NaN      서울특별시              4      GS글로벌      1250      상품 종합 도매업      수출입업(시멘트,철강금속,전기전자,섬유,기계화학),상품중개,광업,채석업/하수처리 서...      1976-06-26      12월      김태형      http://www.gsgcorp.com      서울특별시      국내 주식시장의 종목코드는 6자리 이다. 예를 들어 ‘DL’의 종목코드는 000210이다.그런데 저장된 종목코드는 앞자리 0이 생략되어 있다. map함수로 6자리를 완성한다.# 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해줌stock_code['종목코드'] = stock_code['종목코드'].map('{:06d}'.format)stock_code.head()                  회사명      종목코드      업종      주요제품      상장일      결산월      대표자명      홈페이지      지역                  0      DL      000210      기타 금융업      지주회사      1976-02-02      12월      전병욱      http://www.dlholdings.co.kr      서울특별시              1      DRB동일      004840      고무제품 제조업      고무벨트(V벨트,콘베이어벨트,평벨트),프라스틱제품 제조,판매      1976-05-21      12월      류영식      http://drbworld.com      부산광역시              2      DSR      155660      1차 비철금속 제조업      합섬섬유로프      2013-05-15      12월      홍석빈      http://www.dsr.com      부산광역시              3      GS      078930      기타 금융업      지주회사/부동산 임대      2004-08-05      12월      허태수, 홍순기 (각자 대표이사)      NaN      서울특별시              4      GS글로벌      001250      상품 종합 도매업      수출입업(시멘트,철강금속,전기전자,섬유,기계화학),상품중개,광업,채석업/하수처리 서...      1976-06-26      12월      김태형      http://www.gsgcorp.com      서울특별시      분석에 필요한 컬럼만 추출하고, 컬럼명을 영어로 수정한다.stock_code = stock_code[['회사명', '종목코드', '업종', '주요제품', '상장일', '결산월']]stock_code = stock_code.rename(columns = {'회사명': 'name', '종목코드': 'code', '업종': 'sectors','주요제품': 'products',                                         '상장일': 'listing_date', '결산월': 'closing_date'})stock_code['listing_date'] = pd.to_datetime(stock_code['listing_date'])stock_code.head()                  name      code      sectors      products      listing_date      closing_date                  0      DL      000210      기타 금융업      지주회사      1976-02-02      12월              1      DRB동일      004840      고무제품 제조업      고무벨트(V벨트,콘베이어벨트,평벨트),프라스틱제품 제조,판매      1976-05-21      12월              2      DSR      155660      1차 비철금속 제조업      합섬섬유로프      2013-05-15      12월              3      GS      078930      기타 금융업      지주회사/부동산 임대      2004-08-05      12월              4      GS글로벌      001250      상품 종합 도매업      수출입업(시멘트,철강금속,전기전자,섬유,기계화학),상품중개,광업,채석업/하수처리 서...      1976-06-26      12월      stock_code.shape(2486, 6)3. 함수로 정리하기def get_krx_code():    url = 'http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&amp;searchType=13'    stock_code = pd.read_html(url, header = 0)[0]    stock_code['종목코드'] = stock_code['종목코드'].map('{:06d}'.format)    stock_code = stock_code[['회사명', '종목코드', '업종', '주요제품', '상장일', '결산월']]    stock_code = stock_code.rename(columns = {'회사명': 'name', '종목코드': 'code', '업종': 'sectors','주요제품': 'products',                                              '상장일': 'listing_date', '결산월': 'closing_date'})    stock_code['listing_date'] = pd.to_datetime(stock_code['listing_date'])        return stock_code",
        "url": "/trading1"
    }
    ,
    
    "foody5": {
        "title": "(세종 부강 맛집) 진성민속촌",
            "author": "km.yu99",
            "category": "",
            "content": "세종시 부강면 감자탕 맛집 진성민속촌  주 소 :  세종 부강면 청연로 125  영업시간 : 05:00 ~ 14:00(월요일 휴무)  전화번호 : 044-277-6262  대표메뉴 : 뼈해장국(8,000원), 감자탕 소(20,000원), 뼈추가(10,000원)뼈해장국이 생각날 때 가끔 찾는 진성민속촌. 세종특별시에 속하긴 하지만 신도시에서 떨어진 부강면에 있다. 세종청사에서 자가용으로 20분 정도 소요된다. 영업시간은 오후 2시까지. 그렇다고 순진하게 오후 13시에 도착해서 식사를 하려고 한다면 허탕칠 것이다. 사장님이 하루에 정해진 양만 판매를 하고 정해진 수량이 다 팔리면 일찍 문을 닫는다. 알게모르게 찾는 사람이 많기 때문에 미리 떠날 채비를 해야한다. 본인도 3번이나 허탕을 치고 주말 8시에 가서 운좋게 식사하고 왔다.응암농공단지에서 부강역으로 가는 도로 우측에 다 허물어져가는 초가집이 하나 있을 것이다. 그 곳이 진성민속촌이다. 입구에 커다란 장독대들이 놓여져있고, 매스컴에 노출된 사장님 사진이 걸려져 있다. 입구에 들어서면 사진에서 봤던 사장님이 반갑게 맞이 하신다. 서빙을 하는 이모님들도 친절하다.단촐한 메뉴판. 직접 담근 막걸리는 포장할 경우 5,000원을 받고, 방문해서 먹으면 공짜다.밑반찬이 먼저 나온다. 뼈해장국 먹을 때 밑반찬은 안먹는지라 신경쓰지 못했는데 깍두기가 별미라고 한다.뼈가 가득들어간 해장국이 나온다. 새벽부터 삶아져서 고깃살이 야들야들하다. 잡내도 안난다. 고기와 국물 맛, 그리고 맵기의 밸런스가 훌륭하다.정신없이 먹다보니 사진을 별로 찍지 못했다. 해장국 한그릇을 먹어보기 위해 3번이나 허탕을 친 보람이 있다.뼈다귀 해장국, 감자탕의 진수를 경험해보고 싶다면 이곳을 추천한다.총 평  음식맛 : ★★★★☆  가성비 : ★★★★★  서비스 : ★★★★☆  접근성 : ★☆☆☆☆",
        "url": "/foody5"
    }
    ,
    
    "foody4": {
        "title": "(경기 하남 맛집) 단밥",
            "author": "km.yu99",
            "category": "",
            "content": "하남 팔당 가정식 백반 전문점 단밥  주 소 :  경기 하남시 검단산로 349-25  영업시간 : 11:00 ~ 21:00(매일)  전화번호 : 031-793-7621  대표메뉴 : ** **참숯돼지불고기반상(13,000원), 고등어구이반상(14,000원)하남 팔당대교 인근에 위치한 가정식 백반 전문점, 단밥. 나름 많은 식도락 경험을 가졌다고 자부한다. 그런데 간혹 그럴때가 있다. 한끼를 대충 때우고 싶진 않은데 식당의 음식들이 내키지 않을 때 말이다. 다수를 위해 뽑아내는 음식이 아니라, 단촐하더라도 오직 나를 위한 반찬과 밥들이 생각날 때가 있다.가게는 팔당대교 인근에 있다. 2021년 현재 도로 공사중이고, 팔당댐을 방문하는 사람들 때문에 주말엔 교통이 번잡하다. 대중교통으로 오긴 힘들겠지만, 가게 앞에 주차공간이 넉넉해 자가용으로 이동하기에는 괜찮다.입구에 들어서면 요일별 국이 포스팅되어있다. 국에 대한 자부심이 있는 듯 하다.주말 이른 아침에 와서 오자마자 자리를 잡을 수 있었다. 테이블 수도 많고, 여유 공간도 충분하다. 날씨가 좋으면 야외 정원에서도 식사할 수 있다. 테이블 간격이 좁으면 식사를 하는 내내 소음에 시달려야 하는 경우도 있다. 여기서는 그런 걱정을 안해도 된다.메뉴는 메인 반찬을 중심으로 하는 반상으로 구성된다. 참숯돼지불고기와 고등어구이 반상, 그리고 아이를 위한 새우볶음밥을 주문했다.에피타이저로 김치전이 먼저 나왔다. 들어간 것이 별로 없어 보이지만 갓구워서 식감이 좋다. 식욕을 끌어 올린다.이제 메인메뉴들이 나왔다. 직원들의 서빙도 훌륭하고 친절하다. 반찬과 밥, 그리고 국이 정갈하게 쟁반에 담겨 나온다. 반찬의 양이 적어 보이지만 하나하나 신경쓴 흔적을 엿볼 수 있다. 뭇국의 국물맛도 훌륭하다.참숯돼지불고기 반상. 양이 부족한 감은 있지만 반찬 가짓수를 생각했을 때 그리 적다고 생각되지도 않는다.고등어구이 반상. 메인반찬외에 나머지 반찬은 모두 같다. 여러명이서 온다면 다른 메뉴를 주문한 뒤 메인 반찬을 함께 먹어도 좋을 것 같다.아이 음식으로 주문한 새우볶음밥약간 외진 곳에 있어서 아쉽지만 정갈한 밥한끼가 생각날 때 가보면 좋은 곳이다. 기분 좋게 주말을 시작하도록 도와준 단밥을 추천한다.총 평  음식맛 : ★★★★☆  가성비 : ★★★☆☆  서비스 : ★★★★☆  접근성 : ★★☆☆☆추 가식사를 마친 후 팔당댐을 가려고 했지만 교통체증 때문에 포기를 했다. 근처에 갈 곳이라곤 스타필드 밖에 없다. 당연히 그쪽 방향도 차가 막혔다. 바로 옆에 우즈베이커리에서 커피 한잔하며 기다리기로 했다.입구는 현수막으로 도배되어 꺼림칙하지만 방대한 종류의 베이커리를 판매하는 곳이다. 커피 맛도 훌륭하다. 윗층으로 올라가면 다양한 미술품과 인테리어를 구경하는 재미도 쏠쏠하다. 식당과 함께 방문해 보는 것도 좋을 것 같다.",
        "url": "/foody4"
    }
    ,
    
    "foody3": {
        "title": "(서울 송파 맛집) 평이담백뼈칼국수",
            "author": "km.yu99",
            "category": "",
            "content": "평이담백뼈칼국수 방이점  주 소 :  서울 송파구 위례성대로18길 31-12  영업시간 : 11:30 ~ 21:00(브레이크타임 15~17)  전화번호 : 02-417-7488  대표메뉴 : 뼈칼국수(10,000원), 비빔칼국수(9,000원), 새우만두(6,000원)이번에  소개할 가게는 망원에 본점을 둔 칼국수 체인이다. 돼지고기 등뼈를 칼국수 육수로 사용해서 색다른 맛을 느낄 수 있기에 추천해본다. 가게는 방이역 4번출구 인근에 위치해있다. 가게 앞에 3~4대 정도 주차할 수 있는 공간이 있다.  체인이라서 실내 인테리어도 깔끔하다.주력메뉴는 뼈칼국수와 비빔국수이다. 가격은 약깐 비싸다고 느껴질 수 있겠다. 하지만 맛이 보장된다면 용납할 수 있을 정도다. 한가지만 주문하기 애매해서 뼈칼국수와 비빔칼국수 하나씩 주문했다.뼈칼국수는 고기가 듬뿍 얹혀져서 나온다. 국물도 가볍지 않다. 개인적으로 퍼진 면발을 좋아하지 않는데, 면발이 상당히 쫄깃하다.뼈에 붙은 고기의 양도 상당하다. 뼈해장국처럼 매운양념이 되어있지 않아서 고기 맛이 다소 심심할 수도 있다. 이 때는 함께 나오는 소스에 고기를 찍어 먹어보자.비빔칼국수. 갈아진 돼지고기와 고소한 콩가루가 들어간다. 양념장이 맵지도 않고 적당하다. 뼈칼국수를 시킬지 비빔국수를 시킬지, 방문할 때마다 고민될 것 같다.여유가 된다면 새우만두도 함께 먹어보자. 4개에 6000원으로 다소 비싸보일 수도 있다. 얇은 만두피 안에 새우가 가득차있어 진한 풍미를 느낄 수있다.날씨가 쌀쌀해지면 뜨끈한 국물이 생각날때가 있다. 순대국의 든든함과 쫄깃한 면발을 함께 경험하고 싶다면 한번 방문해보자.총 평  음식맛 : ★★★☆☆  가성비 : ★★★☆☆  서비스 : ★★★☆☆  접근성 : ★★★★☆",
        "url": "/foody3"
    }
    ,
    
    "foody2": {
        "title": "(서울 송파 맛집) 차고버거",
            "author": "km.yu99",
            "category": "",
            "content": "방이동 먹자골목 수제버거 전문점. 차고버거  주 소 :  서울시 송파구 오금로 61-11  영업시간 : 11:30 ~ 22:00(목~금, 브레이크타임 15~17), 11:30 ~ 22:00(토), 11:30 ~ 21:00(일), 월요일 휴무  전화번호 : 0507-1410-0601  대표메뉴 : ** **차고 클래식(7,900원), 하와이안(8,900원), 에그룸(9,900원), 칠리 치즈 프라이(8,800원)내가 사는 방이동 인근에는 식당들이 많다. 석촌 호수를 끼고 조성된 까페들과 양식집들. 최근에 사람들이 몰리고 있는 송리단길. 그리고 오래 전부터 주점들이 들어서 있었던 먹자골목 까지.오늘 소개할 곳은 방이동 먹자골목에 위치한 차고버거. 먹자골목 메인 식당가에서는 떨어져 있다. 개인적으로는 이 위치가 더 마음에 든다. 지하철역에서 가깝고 사람들로 붐비지 않아 편하게 이동할 수 있다.이 가게는 포장이나 배달도 가능하지만 본연의 맛을 느끼기 어렵다. 햄버거는 식어버리면 맛이 급격하게 떨어진다. 특히 프렌치 프라이가 박스안에 같혀서 오래 있으면 고소하고 바삭거리는 식감을 내기 힘들다. 오늘은 큰맘먹고 매장에서 식사를 했다.입구는 넓어 보이지만 매장안에 테이블은 얼마 안된다. 입구 쪽을 확장해서 테이블을 몇개 더 두었다.에그롬과 로코모코, 그리고 하우스커피 한잔을 주문했다.에그룸에는 패티, 베이컨, 계란 후라이가 들어간다. 일반적인 버거 내용물에 계란이 들어 가서 그런지 부드러운 맛이 더해졌다. 하지만 이 구성에 단품 9,900원은 약간 부담스럽다.괌 여행을 갔을 때 처음 먹어본 로코모코. 기본 이상은 해준다. 남자 기준으로 양은 약간 부족한 편.양이 부족한 것 같아 치즈 프라이도 하나 주문했다. 포장해왔을 때는 눅눅해져서 다 먹지 못했는데 확실히 매장에서 먹는 것이 맛있다.요즘 수제버거 가게가 많이 생겨서 흔한 맛이지만, 고깃집과 술집이 대부분인 방이동 먹자골목에서는 흔치 않은 소중한 식당이다.총 평  음식맛 : ★★★☆☆  가성비 : ★★★☆☆  서비스 : ★★★☆☆  접근성 : ★★★★☆",
        "url": "/foody2"
    }
    ,
    
    "foody1": {
        "title": "(서울 과천 맛집) 메밀장터",
            "author": "km.yu99",
            "category": "",
            "content": "지하철 4호선 선바위역에 위치한 막국수 맛집. 메밀장터  주 소 :  경기 과천시 뒷골로 5-7 선바위  영업시간 : 10:40 ~ 21:20(매일)  전화번호 : 02-504-0122  주요메뉴 : 들기름 막국수(9,000원), 명태회 막국수(9,000원)지하철역 인근에 있어서 도보로 이동가능하다.점심시간을 피해 오후 2시 이후에 방문하면 기다리지 않고 입장할 수 있다.아래 사진의 통로로 들어오면 가게 앞 주차장이 있다. 주차공간은 넓지 않은 편이다.사람들이 몰리는 시간대를 피해서 갔지만 주차할 공간이 없어서 인근에 주차해야 했다.식당 안에는 앉아서 식사할 수 있는 방과 테이블이 있다. 테이블은 만석이라 방으로 이동했다.이 곳의 대표 메뉴는 들기름 막국수이다. 개인적으로 비빔국수를 좋아해서 들기름 막국수, 명태회 막국수 하나씩 주문했다.비주얼은 나름 괜찮다. 면 색깔로 보아 메밀이 많이 포함된 것 같아 보이진 않았다.들기름 막국수. 계란 반쪽과 들깨 가루 고명, 들기름이 양념의 전부이다. 과연 이것으로 맛을 낼수 있을까?국수를 잘 비벼서 한젓갈 입에 넣어본다. 입안에 들기름의 고소함이 퍼진다. 지금까지 맛본 들기름 중에 단연 고소함이 으뜸이다.명태회 막국수도 괜찮다. 명태회가 충분이 들어있고 양념장도 간이 잘되어 있다.하지만 역시 이집의 으뜸은 들기름 막국수. 호불호가 갈릴수도 있지만, 두명이서 온다면 들기름, 명태회 막국수 하나씩 시켜서 쉐어하는 것도 괜찮아 보인다. 들기름 국수 면을 90% 정도 건저 먹은 후, 함께 나오는 동치미 국물을 조금 부어서 먹어보자. 이 또한 색다른 맛이다.물 막국수, 비빔 막국수에 익숙한 나에게 들기름 막국수의 세계로 안내한 가게였다.총 평  음식맛 : ★★★★☆  가성비 : ★★★☆☆  서비스 : ★★★☆☆  접근성 : ★★★★☆",
        "url": "/foody1"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://sguys99.github.io//">Happy Plant</a> &copy; 2022</section>
                <section class="poweredby">Published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    <a href="https://facebook.com/dbrhkdaud" target="_blank" rel="noopener">Facebook</a>
                    <!--
                    <a href="https://twitter.com/sguys99" target="_blank" rel="noopener">Twitter</a>
                    -->
                    <a href="https://www.linkedin.com/in/kmyu99" target="_blank" rel="noopener">LinkedIn</a>
                    <a href="https://github.com/sguys99" target="_blank" rel="noopener">Github</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-overlay-close" href="#"></a>
        <div class="subscribe-overlay-content">
            
            <h1 class="subscribe-overlay-title">Search Happy Plant</h1>
            <p class="subscribe-overlay-description">
                lunr.js를 이용한 posts 검색 </p>
            <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()"
               id="searchtext" type="text" name="searchtext"
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>

        </div>
    </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-826PCGZRG1', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
