<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!--custom.css-->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />

    <!-Font Awesome-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <!--웹폰트 추가-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300&display=swap" />

    <!--syntax.css 추가-->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />


    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Data science, Machine learning, and Automatic control" />
    <link rel="shortcut icon" href="https://sguys99.github.io//assets/images/logo.png" type="image/png" />
    <link rel="canonical" href="https://sguys99.github.io//search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Happy Plant" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="Data science, Machine learning, and Automatic control" />
    <meta property="og:url" content="https://sguys99.github.io//search" />
    <meta property="og:image" content="https://sguys99.github.io//assets/images/main-cover.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/dbrhkdaud" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="Data science, Machine learning, and Automatic control" />
    <meta name="twitter:url" content="https://sguys99.github.io//" />
    <meta name="twitter:image" content="https://sguys99.github.io//assets/images/main-cover.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Happy Plant" />
    <meta name="twitter:site" content="@sguys99" />
    <meta name="twitter:creator" content="@sguys99" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Happy Plant",
        "logo": "https://sguys99.github.io//"
    },
    "url": "https://sguys99.github.io//search",
    "image": {
        "@type": "ImageObject",
        "url": "https://sguys99.github.io//assets/images/main-cover.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://sguys99.github.io//search"
    },
    "description": "Data science, Machine learning, and Automatic control"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://sguys99.github.io//">Happy Plant</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-data-science" role="menuitem"><a href="/tag/data-science/">Data Science</a></li>
    <li class="nav-algorithmic-trading" role="menuitem"><a href="/tag/algorithmic-trading/">Trading</a></li>
    <li class="nav-python" role="menuitem"><a href="/tag/it-tips/">It Tips</a></li>
    <li class="nav-diary" role="menuitem"><a href="/tag/diary/">Diary</a></li>
    <!-- 주석
    <li class="nav-goto-turtles3040" role="menuitem"><a href="https://github.com/turtles3040">turtles3040</a></li>
    -->
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Posts by Tag</a>
    </li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
                <a class="social-link social-link-fb" href="https://facebook.com/dbrhkdaud" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>
</a>
            
            
            <a class="social-link social-link-tw" href="https://www.github.com/sguys99" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-github" viewBox="0 0 16 16">
  <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
</svg></a>
            
            
            <a class="social-link social-link-tw" href="https://www.linkedin.com/in/kmyu99" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1"  width="24" height="24" viewBox="0 0 24 24"><path fill="#FFFFFF" d="M21,21H17V14.25C17,13.19 15.81,12.31 14.75,12.31C13.69,12.31 13,13.19 13,14.25V21H9V9H13V11C13.66,9.93 15.36,9.24 16.5,9.24C19,9.24 21,11.28 21,13.75V21M7,21H3V9H7V21M5,3A2,2 0 0,1 7,5A2,2 0 0,1 5,7A2,2 0 0,1 3,5A2,2 0 0,1 5,3Z" /></svg></a>
            
            
                <a class="social-link social-link-tw" href="https://twitter.com/sguys99" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "trading03": {
        "title": "네이버에서 일별 시세 가져오기",
            "author": "km.yu99",
            "category": "",
            "content": "네이버에서는 국내 주식과 관련된 다양한 정보를 제공한다. 본 포스트에서는 네이버 금융에서 일별 주식 시세를 가져오는 방법을 소개한다.1. 홈페이지 둘러보기네이버 금융에 접속한다.  https://finance.naver.com/여기서는 삼성전자를 예로 설명한다. 종목명에 삼성전자를 입력하고 검색버튼을 클릭한다.아래에 시세 버튼을 클릭하면 시간별시세와 일별시세를 확인할 수 있다.우리의 관심항목은 일별시세이다.삼성전자 시세 항목의 url도 살펴보자.  https://finance.naver.com/item/sise.naver?code=005930get 방식으로 서버에 시세정보를 요청하는 형태이다. 파라미터로 종목코드를 사용한다.(삼성전자의 종목코드는 005930)크롬에서 일별시세 부분을 마우스 우클릭하고 검사를 클릭한다.개발자 도구 창이 열리면 Network 탭을 선택하자.일별 시세 아래에 2번 페이지를 클릭해본다. sise_day.naver?code=005930&amp;page=2라는 항목이 새로 생성되었다.이 항목의 헤더를 살펴보면 일별 시세를 가져오고자 하는 서버 url과 파라미터, 그리고 요청 방식을 확인할 수 있다.  서버 주소 : https://finance.naver.com/item/sise_day.naver  파라미터 : code, page  요청방식 : GET사실 get방식으로 url에 입력하면 일별 시세 항목을 조회할 수 있다.예를 들어 삼성전자(005930) 시세의 첫번째 페이지를 조회하고 싶다면 다음과 같이 url을 작성하여 입력하면 된다.  https://finance.naver.com/item/sise_day.naver?code=005930&amp;page=1전체 페이지에 기록된 시세를 가져오려면 마지막 페이지를 번호를 알아야 한다.마우스로 맨뒤 항목을 가리킨 후, 마우스 우클릭-검사를 선택하자.마지막 페이지의 url은 pgRR라는 클래스의 td 태그 내에 정의되어 있다.  https://finance.naver.com/item/sise_day.naver?code=005930&amp;page=641즉, 마지막 페이지 번호는 641이다.이로써 일별 시세를 가져오기 위한 확인과정은 모두 끝났다.2. 일별시세 추출하기여기서는 requests와 BeautifulSoup을 사용해서 추출하는 방법을 설명한다.import pandas as pdfrom bs4 import BeautifulSoupimport requestsfrom datetime import datetime2.1 마지막 페이지 번호 찾기먼저 마지막 페이지를 찾는 방법을 설명한다.code = '005930' # 삼성전자 종목코드url = f\"http://finance.naver.com/item/sise_day.nhn?code={code}\"headers = {'User-agent': 'Mozilla/5.0'} # 웹브라우저 접속처럼 인식시키기 위해 정보 추가서버 주소에 종목 코드를 추가하여 get 양식을 완성하고 요청한다.req = requests.get(url=url, headers = headers)req.text'\\n&lt;html lang=\"ko\"&gt;\\n&lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=euc-kr\"&gt;\\n&lt;title&gt;네이버 금융&lt;/title&gt;\\n\\n&lt;link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/imgstock/static.pc/20211216210327/css/newstock.css\"&gt;\\n&lt;link rel=\"stylesheet\" type=\"text/css\" href=\"https://ssl.pstatic.net/imgstock/static.pc/20211216210327/css/common.css\"&gt;\\n&lt;link ----(생 략)BeautifulSoup으로 추출한 내용을 정리한다.bs = BeautifulSoup(req.text, 'html.parser')bs    &lt;html lang=\"ko\"&gt;    &lt;head&gt;    &lt;meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/&gt;    &lt;title&gt;네이버 금융&lt;/title&gt;    &lt;link href=\"https://ssl.pstatic.net/imgstock/static.pc/20211216210327/css/newstock.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;        --- (생 략)앞에서 마지막 페이지의 url은 pgRR라는 클래스의 td태그 내에 정의되어 있는 것을 확인했다.이 정보를 바탕으로 마지막 페이지를 찾을 수 있다.pgrr = bs.find('td', class_='pgRR')print(pgrr)    &lt;td class=\"pgRR\"&gt;    &lt;a href=\"/item/sise_day.nhn?code=005930&amp;amp;page=641\"&gt;맨뒤    \t\t\t\t&lt;img alt=\"\" border=\"0\" height=\"5\" src=\"https://ssl.pstatic.net/static/n/cmn/bu_pgarRR.gif\" width=\"8\"/&gt;    &lt;/a&gt;    &lt;/td&gt;pgrr.a[\"href\"].split('=')['/item/sise_day.nhn?code', '005930&amp;page', '641']last_page = int(pgrr.a[\"href\"].split('=')[-1])last_page6412.2 일별시세 추출하기일별 시세는 http://finance.naver.com/item/sise_day.nhn에 code와 page 정보를 추가하여 get으로 요청하면 된다.우선 첫번째 페이지만 추출해보자.page_url = '{}&amp;page={}'.format(url, 1)page_url'http://finance.naver.com/item/sise_day.nhn?code=005930&amp;page=1'data = pd.read_html(requests.get(page_url, headers={'User-agent': 'Mozilla/5.0'}).text)[0]data                  날짜      종가      전일비      시가      고가      저가      거래량                  0      NaN      NaN      NaN      NaN      NaN      NaN      NaN              1      2021.12.27      80200.0      300.0      80600.0      80600.0      79800.0      10751648.0              2      2021.12.24      80500.0      600.0      80200.0      80800.0      80200.0      12086380.0              3      2021.12.23      79900.0      500.0      79800.0      80000.0      79300.0      13577498.0              4      2021.12.22      79400.0      1300.0      78900.0      79400.0      78800.0      17105892.0              5      2021.12.21      78100.0      1000.0      77900.0      78300.0      77500.0      14245298.0              6      NaN      NaN      NaN      NaN      NaN      NaN      NaN              7      NaN      NaN      NaN      NaN      NaN      NaN      NaN              8      NaN      NaN      NaN      NaN      NaN      NaN      NaN              9      2021.12.20      77100.0      900.0      77600.0      77800.0      76800.0      11264375.0              10      2021.12.17      78000.0      200.0      76800.0      78000.0      76800.0      13108479.0              11      2021.12.16      77800.0      200.0      78500.0      78500.0      77400.0      11996128.0              12      2021.12.15      77600.0      600.0      76400.0      77600.0      76300.0      9584939.0              13      2021.12.14      77000.0      200.0      76500.0      77200.0      76200.0      10976660.0              14      NaN      NaN      NaN      NaN      NaN      NaN      NaN      주말이나 휴일 같이 장이 열리지 않은 날은 null 값이 들어가있다.이제 10개 페이지를 추출해보자. 전체 페이지 수와 10을 비교해서 작은 값을 추출할 페이지 수(pages)로 지정한다.page_no = 10pages = min(last_page, page_no) # 마지막 페이지와 가져올 페이지 수 중에 작은 값 선택루프를 돌면서 각 페이지의 일별 시세를 추출하여 병합한다.df = pd.DataFrame()for page in range(1, pages+1):    page_url = '{}&amp;page={}'.format(url, page)    df = df.append(pd.read_html(requests.get(page_url, headers={'User-agent': 'Mozilla/5.0'}).text)[0])추출한 시세의 컬럼명을 수정하고, 데이터 타입 변경, 컬럼 순서를 조정한다.df = df.rename(columns={'날짜':'date','종가':'close','전일비':'diff'                ,'시가':'open','고가':'high','저가':'low','거래량':'volume'}) #영문으로 컬럼명 변경df['date'] = pd.to_datetime(df['date']) df = df.dropna() # 결측치 제거df[['close', 'diff', 'open', 'high', 'low', 'volume']] = df[['close','diff', 'open', 'high', 'low', 'volume']].astype(int) # BIGINT형으로 지정한 컬럼을 int형으로 변경df = df[['date', 'open', 'high', 'low', 'close', 'diff', 'volume']]df = df.sort_values(by = 'date') # 날짜순으로 정렬df.head()                  date      open      high      low      close      diff      volume                  13      2021-08-02      79200      79500      78700      79300      800      11739124              12      2021-08-03      79400      81400      79300      81400      2100      24339360              11      2021-08-04      82200      83100      81800      82900      1500      25642368              10      2021-08-05      83300      83300      82000      82100      800      18485469              9      2021-08-06      81900      82500      81300      81500      600      13342623      3. 함수로 정리하기def get_krx_code(market=None):    market_type = ''    if market == 'kospi':        market_type = '&amp;marketType=stockMkt'    elif market == 'kosdaq':        market_type = '&amp;marketType=kosdaqMkt'    elif market == 'konex':        market_type = '&amp;marketType=konexMkt'            url = 'http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&amp;searchType=13{0}'.format(market_type)    stock_code = pd.read_html(url, header = 0)[0]    stock_code['종목코드'] = stock_code['종목코드'].map('{:06d}'.format)    stock_code = stock_code[['회사명', '종목코드', '업종', '상장일']]    stock_code = stock_code.rename(columns = {'회사명': 'name', '종목코드': 'code', '업종': 'sectors',                                              '상장일': 'listing_date'})    stock_code['listing_date'] = pd.to_datetime(stock_code['listing_date'])        return stock_codedef get_stock_price(code, num_of_pages, sort_date = True):    url = f\"http://finance.naver.com/item/sise_day.nhn?code={code}\"    headers = {'User-agent': 'Mozilla/5.0'}     bs = BeautifulSoup(requests.get(url=url, headers = headers).text, 'html.parser')    pgrr = bs.find(\"td\", class_=\"pgRR\")    last_page = int(pgrr.a[\"href\"].split('=')[-1])        pages = min(last_page, num_of_pages) # 마지막 페이지와 가져올 페이지 수 중에 작은 값 선택    df = pd.DataFrame()    for page in range(1, pages+1):        page_url = '{}&amp;page={}'.format(url, page)        df = df.append(pd.read_html(requests.get(page_url, headers={'User-agent': 'Mozilla/5.0'}).text)[0])            df = df.rename(columns={'날짜':'date','종가':'close','전일비':'diff'                ,'시가':'open','고가':'high','저가':'low','거래량':'volume'}) #영문으로 컬럼명 변경    df['date'] = pd.to_datetime(df['date'])     df = df.dropna()    df[['close', 'diff', 'open', 'high', 'low', 'volume']] = \\                            df[['close','diff', 'open', 'high', 'low', 'volume']].astype(int) # int형으로 변경    df = df[['date', 'open', 'high', 'low', 'close', 'diff', 'volume']] # 컬럼 순서 정렬    df = df.sort_values(by = 'date') # 날짜순으로 정렬        if sort_date:        df = df.reset_index(drop = True)        return dfitem_name = '삼성전자'stock = get_krx_code().query(\"name=='{}'\".format(item_name))['code'].to_string(index=False) df = get_stock_price(stock, 10)df.head()                  date      open      high      low      close      diff      volume                  0      2021-08-02      79200      79500      78700      79300      800      11739124              1      2021-08-03      79400      81400      79300      81400      2100      24339360              2      2021-08-04      82200      83100      81800      82900      1500      25642368              3      2021-08-05      83300      83300      82000      82100      800      18485469              4      2021-08-06      81900      82500      81300      81500      600      13342623      ",
        "url": "/trading03"
    }
    ,
    
    "it03": {
        "title": "git cheat sheet3",
            "author": "km.yu99",
            "category": "",
            "content": "이 포스트는 여러 절로 구성되어 있습니다.   cheat sheet1 - 기본 명령어  cheat sheet2 - commit 조작  cheat sheet3 - branch 조작git에 대한 자세한 내용은 다음 책을 참고 자란다.본 절에서는 브랜치 작업과 관련된 명령어를 정리하였다.1. Setup  git branch : 브랜치 목록 표시  git branch [브랜치명] : 해당 브랜치 명으로 브랜치 생성  git checkout [브랜치명] : 해당 브랜치로 전환  git checkout –b [브랜치명] : 브랜치 생성과 동시에 전환  git branch -m [브랜치명] [새로운 브랜치명] : 브랜치명 변경  git branch –d [브랜치명] : 해당 브랜치 삭제2. Merge, rewrite2.1 merge  git merge [브랜치명] : 현 브랜치에 해당 브랜치의 내용 병합  git merge --ff [브랜치명] : fast-forward 관계에 있으면 commit을 생성하지 않고 현재 브랜치의 참조 값 만 변경(default)  git merge --no-ff [브랜치명] : fast-forward 관계에 있어도 merged commit 생성  git merge --squash [브랜치명] : fast-forward 관계에 있어도 merged commit 생성, merging 브랜치 정보 생략2.2 rebase  git rebase [브랜치명] : 현재 브랜치가 해당 브랜치(브랜치명)에부터 분기하도록 재배치  git rebase --continue : 충돌 수정 후 재배치 진행(commit 대신)  git rebase --abort : rebase 취소2.3 cherry-pick  git cherry-pick [commit hash] : 해당 commit의 내용을 현재 브랜치에 추가. 뒤에 commit hash 를 연속 입력하면 복수 추가 가능  git cherry-pick [commit hash start].. [commit hash end] : 해당 구간의 commit을 한번에 추가  git cherry-pick –-abort :  충돌과 같은 상황이 발생했을 때 cherry-pick 취소  git cherry-pick –-continue : 충돌 상황 해결 후 cherry-pick 진행  git cherry-pick –m [parent number] [merge commit ID] : merge commit을 추가. merge commit의 경우 어떤 부분의 merge를 가져올지 알 수 없다. 그래서 parent number를 추가해야 한다.(1부터 시작하며 main line이 1)",
        "url": "/it03"
    }
    ,
    
    "it02": {
        "title": "git cheat sheet2",
            "author": "km.yu99",
            "category": "",
            "content": "이 포스트는 여러 절로 구성되어 있습니다.   cheat sheet1 - 기본 명령어  cheat sheet2 - commit 조작  cheat sheet3 - branch 조작git에 대한 자세한 내용은 다음 책을 참고 자란다.본 포스트에서는 commit 조작과 관련된 명령어를 정리하였다.1. Checkout  git checkout [commit hash] : 해당 commit으로 파일상태 변경  git checkout - : HEAD가 이전에 참조했던 commit으로 상태변경  git checkout master : HEAD가 master를 참조  git checkout HEAD~n : HEAD를 기준으로 n단계 이전 commit으로 상태변경2. Undoing checkout  git reset : Staging area의 파일 전체를 unstaged 상태로 되돌리기  git reset [파일명] : 해당 파일을 unstaged 상태로 되돌리기  git commit --amend : 최근 커밋을 수정하기  git commit --amend -m \"[commit 메시지]\" : 해당 메시지로 commit 수정하기  git reset [commit hash] : 해당 commit으로 브랜치의 참조를 변경  git reset –-hard [commit hash] : working directory, staging area, commit 모두 reset  git reset –-mixed [commit hash] : working directory 유지, staging area, commit reset , default option  git reset –-soft [commit hash] : working directory, staging area 유지, commit reset  git reset HEAD^ : HEAD를 기준으로 직전의 commit으로 reset  git reset HEAD~[정수] : HEAD를 기준으로 정수 값 단계 전 commit으로 reset",
        "url": "/it02"
    }
    ,
    
    "it01": {
        "title": "git cheat sheet1",
            "author": "km.yu99",
            "category": "",
            "content": "이 포스트는 여러 절로 구성되어 있습니다.   cheat sheet1 - 기본 명령어  cheat sheet2 - commit 조작  cheat sheet3 - branch 조작git에 대한 자세한 내용은 다음 책을 참고 자란다.본 포스트에서는 git 기본 명령어와 옵션 별 기능을 정리하였다.1. Setup  git init : 저장소(repository) 생성  git clone [원격 저장소 url] : 해당 주소의 내용을 복제하여 저장소 생성  git config user.name [작성자 이름] : 작성자 이름 설정  git config user.email [이메일 계정] : 작성자 이메일 설정  git config --list : 저장소 설정 전체 출력  git config --get [설정항목] : 일부 설정항목만 출력(ex : git config –get user.name)  git help [커맨드 이름] : 도움말2. Stage &amp; commit  git add [파일이름] : 수정된 파일을 staging area 올리기  git add [디렉토리 명] : 해당 디렉토리 내에 수정된 모든 파일들을 staging area에 올리기  git add . : working directory 내에 수정된 모든 파일들을 staging area에 올리기 (untracked 파일 제외)  git commit : 이력 저장(commit)  git commit -m \"[메시지]\" : vim을 사용하지 않고 인라인으로 메시지를 추가하여 commit  git commit -am \"[메시지]\" : add와 commit을 일괄적으로 진행3. Inspectgit status  git status : 저장소 파일의 상태정보 출력  git status -s : 파일 상태정보를 간략하게 표시git log  git log : 저장소의 commit이력을 출력  git log --pretty=oneline : 각 commit을 한줄로 출력(–pretty 옵션 사용)  git log --oneline : 각 commit을 한줄로 출력  git log --decorate=full : 브랜치나 태그정보를 상세히 출력  git log --graph : 그래프 형태로 출력git show  git show : 가장 최근의 commit 정보 출력  git show [commit hash] : 해당 commit의 정보 출력  git show HEAD : HEAD가 참조하는 commit의 정보 출력  git show HEAD^^^ : HEAD를 기준으로 3단계 이전의 commit정보 출력  git show HEAD~[n] : HEAD를 기준으로 n단계 이전의 commit정보 출력git diff  git diff : 최근 commit과 변경사항이 발생한(Unstaged) 파일들의 내용비교  git diff --staged : 최근 commit과 Staging area의 파일들 간의 변경사항 출력  git diff [commit hash1] [commit hash2] : 두 commit의 파일들 간의 변경사항 출력",
        "url": "/it01"
    }
    ,
    
    "ds02": {
        "title": "custom dataset으로 YOLOv5 학습하기2",
            "author": "km.yu99",
            "category": "",
            "content": "이 포스트는 여러 절로 구성되어 있습니다.     YOLO5 #1 - custom dataset으로 학습하기    YOLO5 #2 - 학습 파라미터 설정하기이전 포스트에서 custom dataset으로 YOLOv5 모델을 학습시키는 방법에 대해서 설명하였다. 여기서는 학습과 관련된 파라미터를 조정하는 방법에 대해서 설명한다. 앞에서와 마찬가지로 실습환경은 google colab이다.1. 데이터셋 소개실습에 사용되는 데이터셋은 roboflow에서 제공되는 Mask Wearking Dataset(raw)이다.[링크](raw와 416x416으로 변환된 데이터셋을 선택할 수 있는데, 여기서는 raw 데이터셋을 사용한다.)2. colab에서 환경구축하기환경구축은 앞과 같기 때문에 간력하게 설명한다. 상세한 내용은 이전 포스트를 참고한다. [링크]  google colab에 접속하고 새 노트를 생성      런타임-런타임 유형 변경을 선택후, 가속기를 GPU로 설정    yolov5 파일을 다운로드 및 필수 라이브러리를 설치!git clone https://github.com/ultralytics/yolov5  # yolov5 코드 clone%cd yolov5 \t\t\t\t\t\t\t\t\t\t  # clone한 폴더로 진입%pip install -qr requirements.txt                 # 필수 라이브러리 설치      custom dataset 업로드 (여기서는 mask_dataset.zip 으로 설명)        데이터 셋 파일 압축 해제  !unzip ../custom_dataset.zip  ` yolov5/data/ 폴더에 mask_dataset.yaml`파일 작성path: /content/yolov5/mask_datasettrain: train/imagesval: valid/imagestest: test/imagesnc: 2names: ['mask', 'no-mask']3. 학습 파라미터 살펴보기이제 학습할 때 파라미터를 조정하는 방법에 대해서 설명한다.모델을 학습할 때 다음과 같이 데이터셋 관련 경로만 입력하면 가능하다.!python train.py --data \"데이터셋.yaml 파일 경로\"나머지 파라미터들은 디펄트 값으로 대체된다. 그러면 학습과 관련된 파라미터에는 어떤 것들이 있을까? yolov5 폴더 안에 있는 train.py 파일을 열어서 440번째 라인 부근에 있는 parse_opt 함수를 살펴보자. 아래와 같이 파라미터들이 정의되어 있다.def parse_opt(known=False):    parser = argparse.ArgumentParser()    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch.yaml', help='hyperparameters path')    parser.add_argument('--epochs', type=int, default=300)    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')    parser.add_argument('--rect', action='store_true', help='rectangular training')        --- (생 략)학습과 관련된 파라미터가 40여개나 된다. 하지만 모두 알 필요는 없다. 모델 성능이나 하드웨어 자원과 관련된 중요한 파라미터만 살펴보자.  모델구조 (–weights)모델 구조와 관련된 파라미터이다. YOLOv5는 다양한 모델 구조를 제공한다. default 값은 YOLOv5로 구조가 제일 간단하다. 모델의 구조가 더 복잡한 것으로 YOLOv5m, YOLOv5l, YOLOv5x 이 있다.구조가 복잡할 수록 성능이 높아질 가능성은 높지만 학습할 때 더 많은 시간이 소요되고 많은 리소스가 요구된다. 예를 들어 yolov5m 모델을 학습시키고 싶다면 다음과 같이 입력하면 된다.--weights \"yolov5m.pt\"공식 github에는 이외에 새로운 모델 구조가 지속적으로 업로드되고 있다. [링크]  배치 사이즈 (–batch-size)학습할 때 한번에 처리할 이미지 수(batch-size)를 지정할 수 있다. default는 16이다. batch size를 32로 입력하고 싶다면 다음과 같이 옵션 설정을 하면된다.--batch-size 32  이미지 크기 (–imgsz,  –img, –img-size)YOLOv5는 학습할 때 모든 이미지 사이즈를 동일하게 resizing 한다. default 사이즈는 640x640이다. 이미지 사이즈를 크게 설정할수록 모델 성능은 더 좋아실 수 있다. 하지만 학습속도와 리소스 부담은 더 커지게 된다. 이미지 크기를 1280x1280으로 설정하고 싶다면 다음과 같이 입력한다.--imgsz(or --img or --img-size) 1280검증이나 시험할 때 학습에 사용한 이미지 사이즈와 동일하게 설정해야한다.  에포크 수 (–epochs)데이터셋으로 학습을 반복할 횟수를 지정하는 에포크의 default 값은 300이다. 100으로 설정하고 싶다면 다음과 같이 입력한다.--epochs 100  하이퍼 파라미터 (–hyp)하이퍼 파라미터가 정의되어 있는 경로를 지정한다. default 값은 data/hyps/hyp.scratch.yaml이다. 해당 경로의 파일을 열어 확인해보자.4. 파라미터를 조정하여 모델 학습하기colab에서 제공하는 자원을 최대한 사용하여 학습을 진행해보자. 모델 구조는 yolov5m.pt, 입력 이미지 크기는 1280, 배치 사이즈는 8, 에포크 수는 60으로 설정해보자. (모델 구조가 커지고 입력 이미지가 복잡해져서 colab gpu 한계를 맞추기 위해 배치 사이즈와 학습시간을 줄여야만 했다.)!python train.py --data \"data/mask_dataset.yaml\" --batch-size 8 --img 1280 --weights \"yolov5m.pt\" --epochs 60train: weights=yolov5m.pt, cfg=, data=data/mask_dataset.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=60, batch_size=8, imgsz=1280, rect=False, --(생 략)hyperparameters: lr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, --(생 략)                 from  n    params  module                                  arguments                       0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]                1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                  2                -1  2     65280  models.common.C3                        [96, 96, 2]   --(생 략)Model Summary: 290 layers, 20856975 parameters, 0 gradients, 48.0 GFLOPs               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:06&lt;00:00,  3.12s/it]                 all         29        162       0.81      0.853      0.861      0.531                mask         29        142      0.871      0.805      0.888      0.559             no-mask         29         20       0.75        0.9      0.835      0.503Results saved to runs/train/exp3여기서는 학습 결과가 /runs/train/exp3에 저장되었다. 사용자 마다 저장위치가 다를 것이다.5. 검증하기검증과 관련된 파라미터는 val.py 파일의 306번째 라인의 parse_opt 함수에 정의되어 있다.학습할 때 이미지 사이즈는 1280으로 설정하였고, 모델 가중치는 /runs/train/exp3/weights/best.pt 저장되어 있으므로 다음과 같이 입력하여 검증을 진행하자.!python val.py --data \"data/mask_dataset.yaml\" --img 1280 --weights \"/content/yolov5/runs/train/exp3/weights/best.pt\"val: data=data/mask_dataset.yaml, weights=['/content/yolov5/runs/train/exp3/weights/best.pt'], batch_size=32, imgsz=1280, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=FalseYOLOv5 🚀 v6.0-159-gdb6ec66 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)Fusing layers... Model Summary: 290 layers, 20856975 parameters, 0 gradients, 48.0 GFLOPsval: Scanning '/content/yolov5/mask_dataset/valid/labels.cache' images and labels... 29 found, 0 missing, 0 empty, 0 corrupted: 100% 29/29 [00:00&lt;?, ?it/s]               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:08&lt;00:00,  8.05s/it]                 all         29        162      0.813      0.851      0.862      0.535                mask         29        142      0.878       0.81      0.887      0.556             no-mask         29         20      0.748      0.892      0.837      0.514Speed: 1.5ms pre-process, 166.8ms inference, 4.4ms NMS per image at shape (32, 3, 1280, 1280)Results saved to runs/val/exp예측결과가 runs/val/exp에 저장되었다.6. 예측하기예측할 때는 기본적으로 모델의 경로(–weights), 입력 데이터 경로(–source)를 지정해줘야 한다. 여기에 추가로 이미지 사이즈(–img)도 지정해주자.!python detect.py --img 1280 --weights \"/content/yolov5/runs/train/exp3/weights/best.pt\" --source \"/content/yolov5/mask_dataset/test/images\"detect: weights=['/content/yolov5/runs/train/exp3/weights/best.pt'], source=/content/yolov5/mask_dataset/test/images, imgsz=[1280, 1280], conf_thres=0.25, ---(생 략)Results saved to runs/detect/expruns/detect/exp에 예측 결과가 저장된다.참 고 : 기회가 되면 confidence threshold(--conf-thres)와 IoU threshold(--iou-thres) 도 변경해가며 예측결과를 비교해보자.",
        "url": "/ds02"
    }
    ,
    
    "ds01": {
        "title": "custom dataset으로 YOLOv5 학습하기1",
            "author": "km.yu99",
            "category": "",
            "content": "이 포스트는 여러 절로 구성되어 있습니다.     YOLO5 #1 - custom dataset으로 학습하기    YOLO5 #2 - 학습 파라미터 설정하기YOLO(You Only Look Once)는 널리 쓰이는 object detection 알고리즘이다. 최근에는 YOLOv5 까지 출시되었다. 여기서는 공식 github 계정에 업로드된 YOLOv5 코드로 custom dataset을 학습하는 방법에 대하여 설명한다. google colab 환경에서 진행되었다.1. 데이터셋 소개실습에 사용되는 데이터셋은 roboflow에서 제공되는 North American Mushrooms Dataset이다.[링크]여기서는 학습시간을 줄이기 위해서 416x416 사이즈의 이미지 51장을 다운 받았다.object detection 알고리즘 라이브러리 구현방식에 따라, 그리고 YOLO 버전 별로도 사용하는 레이블링 파일의 포맷이 다르다. roboflow에서는 레이블링 파일 포맷을 선택하여 다운도르 할 수 있다. 우리는 PyTorch로 구현된 공식 계정의 코드를 사용할 예정이므로 YOLO v5 PyTorch를 선택하고 다운로드 한다.참고로 YOLOv5 공식계정의 코드는 txt 포맷의 레이블링 데이터를 사용한다.이 파일은 이미지에서 검출된 object에 대한 클래스와 bounding box 정보를 포함하고 있다. 검출 객체정보 배치는 [class, x_center, y_center, width, height] 형태로 되어있다. bounding box 정보는 이미지 사이즈에 의해 정규화 되어있다. 따라서 0~1 범위의 값을 가진다.편의를 위해 다운로드한 데이터 셋 압축파일의 폴더 이름을 custom_dataset으로 수정한다. 데이터 폴더 구성은 다음과 같다.custom_dataset│├── test/│   ├── images/                    │   └── labels/             │├── train/│   ├── images/                │   └── labels/               │├── valid/│   ├── images/                    │   └── labels/   │├── data.yaml└── README.dataset.txtdata.yaml파일을 메모장으로 열어보자. 데이터 셋 기본정보가 포함되어 있다. 우리는 *.yaml 파일을 새로 만들 것이다. 어떤 식으로 구성되는지 참고만 한다.train과 val은 각 데이터 셋의 경로정보이다. 그리고 nc는 class의 수(number of classes)를, names는 각 클래스의 이름이다.2. colab에서 환경구축하기google colab에 접속하고 새 노트를 생성한다. 런타임-런타임 유형 변경을 선택하여, 하드웨어 가속기를 GPU로 설정한다.이제 colab 노트에 공식 github 계정의 파일을 다운로드하고, 필수 라이브러리를 설치하는 명령을 입력한다.!git clone https://github.com/ultralytics/yolov5  # yolov5 코드 clone%cd yolov5 \t\t\t\t\t\t\t\t\t\t  # clone한 폴더로 진입%pip install -qr requirements.txt                 # 필수 라이브러리 설치Cloning into 'yolov5'...remote: Enumerating objects: 10354, done.remote: Total 10354 (delta 0), reused 0 (delta 0), pack-reused 10354Receiving objects: 100% (10354/10354), 10.58 MiB | 23.75 MiB/s, done.Resolving deltas: 100% (7149/7149), done./content/yolov5     |████████████████████████████████| 596 kB 5.4 MB/s 파일 탐색기에 yolov5 폴더가 생성되었고, 파일들이 다운로드 되어있다.이제 앞에서 다운로드한 데이터셋을 업로드 한다. 파일 탐색기의 업로드 아이콘을 클릭하여 custom_dataset.zip 파일을 업로드 한다.업로드가 완료되면 탐색기에 해당 파일이 표시된다.unzip 명령으로 데이터 셋 파일의 압축을 해제한다.!unzip ../custom_dataset.zipArchive:  ../custom_dataset.zip  inflating: custom_dataset/data.yaml    inflating: custom_dataset/README.dataset.txt    inflating: custom_dataset/README.roboflow.txt     creating: custom_dataset/test/   creating: custom_dataset/test/images/  inflating: custom_dataset/test/images/chanterelle_02_jpg.rf.f7a48494b7393c532f641585d99a57be.jpg    inflating: custom_dataset/test/images/chanterelle_03_jpg.rf.580f8d787af6a8050c21c065bf016f20.jpg    inflating: custom_dataset/test/images/chanterelle_03_jpg.rf.cd892d2f06d228ba20d194fc360320fc.jpg    --- (생 략)완료되면 yolov5/custom_dataset/ 경로에 데이터 셋이 위치하게 된다. (현 작업 디렉토리가 yolov5이기 때문)마지막으로 데이터 셋 설정파일을 작성한다.` yolov5/data/ 폴더에 custom_dataset.yaml`이라는 이름의 파일을 생성한다.여기에 다음과 같이 설정정보를 입력한다.path: /content/yolov5/custom_dataset  #root 디렉토리train: train/images\t\t\t\t\t  # 학습데이터 경로val: valid/imagestest: test/imagesnc: 2\t\t\t\t\t\t\t\t# 클래스 수names: ['CoW', 'chanterelle']\t\t# 클래스 이름자세한 내용은 다음 링크의 **1.1 Create dataset.yaml** 항목을 참고하자.이로써 학습을 위한 모든 준비가 완료 되었다.3. 모델 학습하기모델 학습 순서는 다음과 같다.위와 같은 일련의 과정은 train.py  파일 실행을 통해 가능하다. 인자로 학습 데이터 경로와 epoch 수를 입력하고 학습을 진행하자.!python train.py --data \"data/custom_dataset.yaml\" --epochs 100 #epoch 100회 Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf... train: weights=yolov5s.pt, cfg=, data=data/custom_dataset.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=100, batch_size=16, imgsz=640, '''  ---(생략)  Overriding model.yaml nc=80 with nc=2                   from  n    params  module                                  arguments                        0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]                 1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                   2                -1  1     18816  models.common.C3                        [64, 64, 1]    ---(생략)  Logging results to runs/train/exp Starting training for 100 epochs...       Epoch   gpu_mem       box       obj       cls    labels  img_size       0/99     3.23G    0.1252   0.03226   0.02699        28       640: 100% 3/3 [00:05&lt;00:00,  1.95s/it]                Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00&lt;00:00,  2.48it/s]                  all          5         14    0.00695      0.311    0.00395     0.0011        ---(생략)  Validating runs/train/exp/weights/best.pt... Fusing layers...  Model Summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs                Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00&lt;00:00,  4.48it/s]                  all          5         14       0.95      0.996      0.973      0.697                  CoW          5          5          1      0.991      0.995      0.688          chanterelle          5          9      0.899          1      0.951      0.706 Results saved to runs/train/exp학습이 완료되면 runs/train/exp경로에 학습 결과가 저장된다. 학습을 반복하면 runs/train경로에 exp1, 2, 3… 같은 형태로 폴더가 생성되면서 학습 결과가 기록된다.학습 결과를 다운로드 하고 싶다면 zip 명령을 압축한 뒤, 저장한다 . 예를 들어 train_result.zip이라는 이름으로 압축하고 싶다면 다음과 같이 입력한다.!zip -r train_result.zip /content/yolov5/runs/train/exp탐색기에 train_result.zip가 표시되면 정상으로 압축된 것이다.4. 학습한 모델 검증하기이제 학습한 모델로 검증을 진행해보자. 검증순서는 앞의 학습 절차에서 모델 가중치 업데이트 과정이 생략된 것이다.모델 검증은 val.py  파일 실행을 통해 진행한다. 다양한 인자가 있지만 데이터 경로(--data), 모델 가중치(--weights) 정도만 입력해서 실행해보자. 앞에서 학습한 모델 가중치는 runs/train/exp/weights/best.pt에 저장되었다.!python val.py --data \"data/custom_dataset.yaml\" --weights \"/content/yolov5/runs/train/exp/weights/best.pt\"val: data=data/custom_dataset.yaml, weights=['/content/yolov5/runs/train/exp/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=FalseYOLOv5 🚀 v6.0-155-gdc54ed5 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)Fusing layers... Model Summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPsval: Scanning '/content/yolov5/custom_dataset/valid/labels.cache' images and labels... 5 found, 0 missing, 0 empty, 0 corrupted: 100% 5/5 [00:00&lt;?, ?it/s]               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00&lt;00:00,  2.58it/s]                 all          5         14      0.909      0.982      0.961      0.686                 CoW          5          5          1      0.965      0.995      0.672         chanterelle          5          9      0.818          1      0.926        0.7Speed: 0.6ms pre-process, 29.3ms inference, 3.0ms NMS per image at shape (32, 3, 640, 640)Results saved to runs/val/exp검증결과는 runs/val/exp에 저장된다. 앞에서와 마찬가지로 다운로드 받고 싶다면 폴더를 압축하자.!zip -r val_result.zip /content/yolov5/runs/valexp 폴더 안에는 confusion matrix, F1 curve 등 성능과 관련된 차트가 저장되어 있다.5. 학습한 모델로 예측하기예측과정은 아래 그림과 같은 절차로 진행된다.예측 과정은 detect.py 파일을 사용한다. 단순 이미지 뿐만 아니라 웹캠, 비디오 파일 등에서도 실행 가능하다. --source인자에 다음과 같이 설정해주면 된다.!python detect.py --source 0  # webcam                            img.jpg  # image                            vid.mp4  # video                            path/  # directory                            path/*.jpg  # glob                            'https://youtu.be/Zgi9g1ksQHc'  # YouTube                            'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream여기서는 custom_dataset/test/images 경로에 있는 이미지에 대해서 object detection을 실행해본다. 인식 대상 (--source), 모델 가중치(--weights)  경로를 입력해서 실행해보자.!python detect.py --weights \"/content/yolov5/runs/train/exp/weights/best.pt\" --source \"/content/yolov5/custom_dataset/test/images\"detect: weights=['/content/yolov5/runs/train/exp/weights/best.pt'], source=/content/yolov5/custom_dataset/test/images, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, ---(생략)Fusing layers... Model Summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPsimage 1/5 /content/yolov5/custom_dataset/test/images/chanterelle_02_jpg.rf.f7a48494b7393c532f641585d99a57be.jpg: 640x640 3 chanterelles, Done. (0.034s)--- (생략)테스트 결과는 /runs/detect/exp 경로에 저장된다. 결과를 다운로드 하고 싶다면 다음과 같이 압축하여 저장한다.!zip -r test_result.zip /content/yolov5/runs/detect/exp폴더를 열어보면 class와 bounding box가 표시된 detection 결과 이미지가 저장되어 있다.",
        "url": "/ds01"
    }
    ,
    
    "trading2": {
        "title": "KRX에서 종목코드 가져오기2",
            "author": "km.yu99",
            "category": "",
            "content": "한국거래소(KRX)에서 증권과 관련된 데이터와 통계정보를 제공하는 정보데이터시스템 웹페이지가 있다.여기서도 상장주식의 종목코드 리스트를 가져올 수 있다.1. 홈페이지 둘러보기KRX의 전자정보데이터시스템에 접속한다.  http://data.krx.co.kr/contents/MDC/MAIN/main/index.cmd‘주식’-‘종목정보’-‘전종목 지정내역’ 또는 [다음] 링크를 클릭한다.해당 화면에서 국내 상장주식의 기본정보를 조회할 수 있다. 우측에 다운로드 아이콘을 클릭하면 원하는 포맷으로 데이터를 다운로드 할수도 있다.크롬에서 ‘F12’버튼 또는 마우스 우클릭 후 ‘검사’를 클릭하여 개발자 도구 창을 연다.상단 탭에서 ‘Network’를 선택한다. 이 상태에서 앞에서 설명한 다운로드 항목에서 ‘CSV’ 아이콘을 클릭하여 파일 다운로드를 실행해본다.개발자 도구창 하단에 ‘generate.cmd’, ‘download.cmd’가 새로 생성되었다.즉, 파일 다운로드 과정은 두단계를 거치게 된다. ‘generate.cmd’에 의해서 데이터를 다운로드 하기 위한 code를 리턴한다. 이 후 ‘download.cmd’로 code를 파라미터로 하여 데이터를 저장한다.‘generate.cmd’를 선택하고 상단의 ‘Headers’를 클릭해보자.요청할 주소와 방식(POST) 등을 확인할 수 있다.‘Payload’를 클릭하면 요청할 때 사용된 파라미터를 확인된다.마찬가지로 ‘download.cmd’를 선택하고 상단의 ‘Headers’를 클릭해보자.요청할 주소와 방식(POST)을 확인할 수 있다.‘Payload’에는 code 값이 표시되어 있다. 이 값은 ‘generate.cmd’로 받아와야 하는 값이다.이 정보를 바탕으로 코드를 작성한다.2. 추출하기웹에서 정보를 가져오기 위해 requests 라이브러리를 사용할 것이다.필요한 라이브러리를 import 한다.import requestsimport pandas as pdfrom io import BytesIO첫번째 단계로 generate 요청을 진행한다.요청할 주소와 파라미터를 설정하자. 앞에 소개한 ‘generate.cmd’에 명시된 내용이다.gen_url = 'http://data.krx.co.kr/comm/fileDn/GenerateOTP/generate.cmd'gen_parms = {    'mktId': 'ALL',    'share': '1',    'csvxls_isNo': 'false',    'name': 'fileDown',    'url': 'dbms/MDC/STAT/standard/MDCSTAT01901'    }python 코드에 의한 접근이 아닌 크롬으로 접근하고 있음을 표시하기 위해 header 정보를 추가한다. ‘Headers’ 하단에 있는 ‘Request Headers’ 항목을 참고하여 작성한다.headers = {    'Referer': 'http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0201020101',    'Upgrade-Insecure-Requests': '1',    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36' #generate.cmd에서 찾아서 입력하세요    }code 값을 리턴받기 위해 요청을 진행한다.r = requests.get(url=gen_url, params=gen_parms, headers=headers)r.contentb'a1n6kaOi+6ccSQWhSJQn6cmnCCnJIeb940e8ATaeiE4RtSksuLS7Bnxpl86F7dAOvXfGx9S2U5wgvoxsacATRRtmGtORI4WrGDmruVe6oXtCqUypoW0Lp6SAPP0PhVkgThCTcjIZNPI5lCTubZnhjio6AHXdxc45YVEhz4JdugHPMxvIwHadpQpCGE1HxZAXvTCprTIXuXT9XxFb88awpQ=='r.content에 code 값이 저장되었다.이제 두번째 단계, 다운로드 요청을 진행한다.‘download.cmd’에 표시된 요청주소를 참고하고, code는 앞에서 받은 값을 지정한다.down_url = 'http://data.krx.co.kr/comm/fileDn/download_csv/download.cmd'data = {    'code': r.content}r = requests.post(url=down_url, data=data, headers=headers)r.content이렇게 받은 정보는 바이너리 데이터이다.이 데이터를 다루기 위해 BytesIO 메서드를 사용한다.이제 pandas의 read_csv 메서드로 데이터를 읽어온다.stock_code = pd.read_csv(BytesIO(r.content), encoding='cp949')stock_code.head()필요한 컬럼만 가져오고 컬럼명을 영문으로 변경한다.stock_code = stock_code[['한글 종목약명', '단축코드', '시장구분', '액면가', '상장주식수']]stock_code = stock_code.rename(columns = {'시장구분': 'market', '한글 종목약명': 'name', '단축코드': 'code',                                           '액면가': 'par_value', '상장주식수': 'total_shrs'})stock_code.head()                  name      code      market      par_value      total_shrs                  0      마이크로컨텍솔      098120      KOSDAQ      500      8312766              1      스카이이앤엠      131100      KOSDAQ      500      11642629              2      포스코엠텍      009520      KOSDAQ      500      41642703              3      AJ네트웍스      095570      KOSPI      1000      46822295              4      AK홀딩스      006840      KOSPI      5000      13247561      3. 함수로 정리하기def get_krx_code():    gen_url = 'http://data.krx.co.kr/comm/fileDn/GenerateOTP/generate.cmd'    gen_parms = {        'mktId': 'ALL',        'share': '1',        'csvxls_isNo': 'false',        'name': 'fileDown',        'url': 'dbms/MDC/STAT/standard/MDCSTAT01901'        }    headers = {    'Referer': 'http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0201020101',    'Upgrade-Insecure-Requests': '1',    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36' #generate.cmd에서 찾아서 입력하세요    }    r = requests.get(url=gen_url, params=gen_parms, headers=headers)        down_url = 'http://data.krx.co.kr/comm/fileDn/download_csv/download.cmd'    data = {        'code': r.content    }    r = requests.post(url=down_url, data=data, headers=headers)        stock_code = pd.read_csv(BytesIO(r.content), encoding='cp949')    stock_code = stock_code[['한글 종목약명', '단축코드', '시장구분', '액면가', '상장주식수']]    stock_code = stock_code.rename(columns = {'시장구분': 'market', '한글 종목약명': 'name', '단축코드': 'code',                                               '액면가': 'par_value', '상장주식수': 'total_shrs'})        return stock_code",
        "url": "/trading2"
    }
    ,
    
    "trading1": {
        "title": "KRX에서 종목코드 가져오기1",
            "author": "km.yu99",
            "category": "",
            "content": "1. 홈페이지 둘러보기먼저 KRX 홈페이지(https://kind.krx.co.kr/) 를 방문하여 ‘상장법인상세정보’/ ‘상장법인목록’ 화면으로 이동한다.(또는 [다음] 링크를 클릭한다.)‘EXECEL’ 버튼을 클릭하면 전체 상장법인 목록을 다운받을 수 있다.다운로드한 파일은 ‘xls’ 확장자를 가지고 있지만 html 포맷이다. 우리는 파이썬 코드로 위 목록을 가져와 pandas 데이터프레임으로 변환하고자 한다.다시 돌아와서 상장법인 목록을 다운로드 하는 방식을 살펴보자.‘시장구분’, ‘검색유형’ 등과 같은 범주를 설정하고 ‘EXCEL’ 버튼을 클릭하면 해당 범주에 해당하는 법인목록이 다운로드 되는 방식이다.설정항목을 수정하지 않고 EXCEL 버튼을 클릭하면 전체 상장법인 목록이 다운로드 된다.크롬에서 해당 페이지를 열고 ‘상장법인목록’ 부분을 마우스 우클릭하고, ‘검사’ 항목을 선택하자.검색 항목의 소스를 확인할 수 있다.요약하면 ‘searchForm’이라고 하는 양식을 통해서 추출 조건을 지정한다. 그리고 ‘post’를 사용해서 데이터 추출을 요청한다.추출 처리를 담당하는 서버의 주소는 ‘http://kind.krx.co.kr/corpgeneral/corpList.do’ 이다.그리고 ‘EXCEL’ 버튼을 클릭했을 때, 동작하는 메커니즘을 살펴보자.버튼을 클릭하면 ‘fnDownolad()’ 함수를 호출한다.이 함수는 searchForm에 설정된 조건대로 데이터를 다운로드 하도록 서버 프로그램에 요청하는 함수이다.2. 추출하기다운되는 종목코드 파일이 html 형식이므로 pandas의 read_html 함수를 사용한다.read_html의 인자에 다운로드를 요청할 url을 지정하면 된다.서버의 url 뒤에 ‘?’로 url의 끝을 표시하고 데이터(검색조건)를 추가하면 된다.주요 조건은 다음과 같다.  method: 메서드, 예시) ‘download’  orderMode:정렬컬럼, 예시) ‘1’,  orderStat: 정렬 내림차순, 예시) ‘D’  searchType: 검색유형, 예시) ‘13’(상장법인)  fiscalYearEnd: 결산원, 예시) ‘all’(전체)  location: 지역, 예시) ‘all’(전체)여기서는 다음과 같이 설정한다. (나머지는 default 값 적용)url = 'http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&amp;searchType=13'import pandas as pdimport numpy as npimport osstock_code = pd.read_html(url, header = 0)[0] # 주의 : [0]을 반드시 추가. read_html은 table들을 읽어 리스트로 저장함stock_code.head()                  회사명      종목코드      업종      주요제품      상장일      결산월      대표자명      홈페이지      지역                  0      DL      210      기타 금융업      지주회사      1976-02-02      12월      전병욱      http://www.dlholdings.co.kr      서울특별시              1      DRB동일      4840      고무제품 제조업      고무벨트(V벨트,콘베이어벨트,평벨트),프라스틱제품 제조,판매      1976-05-21      12월      류영식      http://drbworld.com      부산광역시              2      DSR      155660      1차 비철금속 제조업      합섬섬유로프      2013-05-15      12월      홍석빈      http://www.dsr.com      부산광역시              3      GS      78930      기타 금융업      지주회사/부동산 임대      2004-08-05      12월      허태수, 홍순기 (각자 대표이사)      NaN      서울특별시              4      GS글로벌      1250      상품 종합 도매업      수출입업(시멘트,철강금속,전기전자,섬유,기계화학),상품중개,광업,채석업/하수처리 서...      1976-06-26      12월      김태형      http://www.gsgcorp.com      서울특별시      국내 주식시장의 종목코드는 6자리 이다. 예를 들어 ‘DL’의 종목코드는 000210이다.그런데 저장된 종목코드는 앞자리 0이 생략되어 있다. map함수로 6자리를 완성한다.# 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해줌stock_code['종목코드'] = stock_code['종목코드'].map('{:06d}'.format)stock_code.head()                  회사명      종목코드      업종      주요제품      상장일      결산월      대표자명      홈페이지      지역                  0      DL      000210      기타 금융업      지주회사      1976-02-02      12월      전병욱      http://www.dlholdings.co.kr      서울특별시              1      DRB동일      004840      고무제품 제조업      고무벨트(V벨트,콘베이어벨트,평벨트),프라스틱제품 제조,판매      1976-05-21      12월      류영식      http://drbworld.com      부산광역시              2      DSR      155660      1차 비철금속 제조업      합섬섬유로프      2013-05-15      12월      홍석빈      http://www.dsr.com      부산광역시              3      GS      078930      기타 금융업      지주회사/부동산 임대      2004-08-05      12월      허태수, 홍순기 (각자 대표이사)      NaN      서울특별시              4      GS글로벌      001250      상품 종합 도매업      수출입업(시멘트,철강금속,전기전자,섬유,기계화학),상품중개,광업,채석업/하수처리 서...      1976-06-26      12월      김태형      http://www.gsgcorp.com      서울특별시      분석에 필요한 컬럼만 추출하고, 컬럼명을 영어로 수정한다.stock_code = stock_code[['회사명', '종목코드', '업종', '주요제품', '상장일', '결산월']]stock_code = stock_code.rename(columns = {'회사명': 'name', '종목코드': 'code', '업종': 'sectors','주요제품': 'products',                                         '상장일': 'listing_date', '결산월': 'closing_date'})stock_code['listing_date'] = pd.to_datetime(stock_code['listing_date'])stock_code.head()                  name      code      sectors      products      listing_date      closing_date                  0      DL      000210      기타 금융업      지주회사      1976-02-02      12월              1      DRB동일      004840      고무제품 제조업      고무벨트(V벨트,콘베이어벨트,평벨트),프라스틱제품 제조,판매      1976-05-21      12월              2      DSR      155660      1차 비철금속 제조업      합섬섬유로프      2013-05-15      12월              3      GS      078930      기타 금융업      지주회사/부동산 임대      2004-08-05      12월              4      GS글로벌      001250      상품 종합 도매업      수출입업(시멘트,철강금속,전기전자,섬유,기계화학),상품중개,광업,채석업/하수처리 서...      1976-06-26      12월      stock_code.shape(2486, 6)3. 함수로 정리하기def get_krx_code():    url = 'http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&amp;searchType=13'    stock_code = pd.read_html(url, header = 0)[0]    stock_code['종목코드'] = stock_code['종목코드'].map('{:06d}'.format)    stock_code = stock_code[['회사명', '종목코드', '업종', '주요제품', '상장일', '결산월']]    stock_code = stock_code.rename(columns = {'회사명': 'name', '종목코드': 'code', '업종': 'sectors','주요제품': 'products',                                              '상장일': 'listing_date', '결산월': 'closing_date'})    stock_code['listing_date'] = pd.to_datetime(stock_code['listing_date'])        return stock_code",
        "url": "/trading1"
    }
    ,
    
    "foody4": {
        "title": "(경기 하남 맛집) 단밥",
            "author": "km.yu99",
            "category": "",
            "content": "하남 팔당 가정식 백반 전문점 단밥  주 소 :  경기 하남시 검단산로 349-25  영업시간 : 11:00 ~ 21:00(매일)  전화번호 : 031-793-7621  대표메뉴 : ** **참숯돼지불고기반상(13,000원), 고등어구이반상(14,000원)하남 팔당대교 인근에 위치한 가정식 백반 전문점, 단밥. 나름 많은 식도락 경험을 가졌다고 자부한다. 그런데 간혹 그럴때가 있다. 한끼를 대충 때우고 싶진 않은데 식당의 음식들이 내키지 않을 때 말이다. 다수를 위해 뽑아내는 음식이 아니라, 단촐하더라도 오직 나를 위한 반찬과 밥들이 생각날 때가 있다.가게는 팔당대교 인근에 있다. 2021년 현재 도로 공사중이고, 팔당댐을 방문하는 사람들 때문에 주말엔 교통이 번잡하다. 대중교통으로 오긴 힘들겠지만, 가게 앞에 주차공간이 넉넉해 자가용으로 이동하기에는 괜찮다.입구에 들어서면 요일별 국이 포스팅되어있다. 국에 대한 자부심이 있는 듯 하다.주말 이른 아침에 와서 오자마자 자리를 잡을 수 있었다. 테이블 수도 많고, 여유 공간도 충분하다. 날씨가 좋으면 야외 정원에서도 식사할 수 있다. 테이블 간격이 좁으면 식사를 하는 내내 소음에 시달려야 하는 경우도 있다. 여기서는 그런 걱정을 안해도 된다.메뉴는 메인 반찬을 중심으로 하는 반상으로 구성된다. 참숯돼지불고기와 고등어구이 반상, 그리고 아이를 위한 새우볶음밥을 주문했다.에피타이저로 김치전이 먼저 나왔다. 들어간 것이 별로 없어 보이지만 갓구워서 식감이 좋다. 식욕을 끌어 올린다.이제 메인메뉴들이 나왔다. 직원들의 서빙도 훌륭하고 친절하다. 반찬과 밥, 그리고 국이 정갈하게 쟁반에 담겨 나온다. 반찬의 양이 적어 보이지만 하나하나 신경쓴 흔적을 엿볼 수 있다. 뭇국의 국물맛도 훌륭하다.참숯돼지불고기 반상. 양이 부족한 감은 있지만 반찬 가짓수를 생각했을 때 그리 적다고 생각되지도 않는다.고등어구이 반상. 메인반찬외에 나머지 반찬은 모두 같다. 여러명이서 온다면 다른 메뉴를 주문한 뒤 메인 반찬을 함께 먹어도 좋을 것 같다.아이 음식으로 주문한 새우볶음밥약간 외진 곳에 있어서 아쉽지만 정갈한 밥한끼가 생각날 때 가보면 좋은 곳이다. 기분 좋게 주말을 시작하도록 도와준 단밥을 추천한다.총 평  음식맛 : ★★★★☆  가성비 : ★★★☆☆  서비스 : ★★★★☆  접근성 : ★★☆☆☆추 가식사를 마친 후 팔당댐을 가려고 했지만 교통체증 때문에 포기를 했다. 근처에 갈 곳이라곤 스타필드 밖에 없다. 당연히 그쪽 방향도 차가 막혔다. 바로 옆에 우즈베이커리에서 커피 한잔하며 기다리기로 했다.입구는 현수막으로 도배되어 꺼림칙하지만 방대한 종류의 베이커리를 판매하는 곳이다. 커피 맛도 훌륭하다. 윗층으로 올라가면 다양한 미술품과 인테리어를 구경하는 재미도 쏠쏠하다. 식당과 함께 방문해 보는 것도 좋을 것 같다.",
        "url": "/foody4"
    }
    ,
    
    "foody3": {
        "title": "(서울 송파 맛집) 평이담백뼈칼국수",
            "author": "km.yu99",
            "category": "",
            "content": "평이담백뼈칼국수 방이점  주 소 :  서울 송파구 위례성대로18길 31-12  영업시간 : 11:30 ~ 21:00(브레이크타임 15~17)  전화번호 : 02-417-7488  대표메뉴 : 뼈칼국수(10,000원), 비빔칼국수(9,000원), 새우만두(6,000원)이번에  소개할 가게는 망원에 본점을 둔 칼국수 체인이다. 돼지고기 등뼈를 칼국수 육수로 사용해서 색다른 맛을 느낄 수 있기에 추천해본다. 가게는 방이역 4번출구 인근에 위치해있다. 가게 앞에 3~4대 정도 주차할 수 있는 공간이 있다.  체인이라서 실내 인테리어도 깔끔하다.주력메뉴는 뼈칼국수와 비빔국수이다. 가격은 약깐 비싸다고 느껴질 수 있겠다. 하지만 맛이 보장된다면 용납할 수 있을 정도다. 한가지만 주문하기 애매해서 뼈칼국수와 비빔칼국수 하나씩 주문했다.뼈칼국수는 고기가 듬뿍 얹혀져서 나온다. 국물도 가볍지 않다. 개인적으로 퍼진 면발을 좋아하지 않는데, 면발이 상당히 쫄깃하다.뼈에 붙은 고기의 양도 상당하다. 뼈해장국처럼 매운양념이 되어있지 않아서 고기 맛이 다소 심심할 수도 있다. 이 때는 함께 나오는 소스에 고기를 찍어 먹어보자.비빔칼국수. 갈아진 돼지고기와 고소한 콩가루가 들어간다. 양념장이 맵지도 않고 적당하다. 뼈칼국수를 시킬지 비빔국수를 시킬지, 방문할 때마다 고민될 것 같다.여유가 된다면 새우만두도 함께 먹어보자. 4개에 6000원으로 다소 비싸보일 수도 있다. 얇은 만두피 안에 새우가 가득차있어 진한 풍미를 느낄 수있다.날씨가 쌀쌀해지면 뜨끈한 국물이 생각날때가 있다. 순대국의 든든함과 쫄깃한 면발을 함께 경험하고 싶다면 한번 방문해보자.총 평  음식맛 : ★★★☆☆  가성비 : ★★★☆☆  서비스 : ★★★☆☆  접근성 : ★★★★☆",
        "url": "/foody3"
    }
    ,
    
    "foody2": {
        "title": "(서울 송파 맛집) 차고버거",
            "author": "km.yu99",
            "category": "",
            "content": "방이동 먹자골목 수제버거 전문점. 차고버거  주 소 :  서울시 송파구 오금로 61-11  영업시간 : 11:30 ~ 22:00(목~금, 브레이크타임 15~17), 11:30 ~ 22:00(토), 11:30 ~ 21:00(일), 월요일 휴무  전화번호 : 0507-1410-0601  대표메뉴 : ** **차고 클래식(7,900원), 하와이안(8,900원), 에그룸(9,900원), 칠리 치즈 프라이(8,800원)내가 사는 방이동 인근에는 식당들이 많다. 석촌 호수를 끼고 조성된 까페들과 양식집들. 최근에 사람들이 몰리고 있는 송리단길. 그리고 오래 전부터 주점들이 들어서 있었던 먹자골목 까지.오늘 소개할 곳은 방이동 먹자골목에 위치한 차고버거. 먹자골목 메인 식당가에서는 떨어져 있다. 개인적으로는 이 위치가 더 마음에 든다. 지하철역에서 가깝고 사람들로 붐비지 않아 편하게 이동할 수 있다.이 가게는 포장이나 배달도 가능하지만 본연의 맛을 느끼기 어렵다. 햄버거는 식어버리면 맛이 급격하게 떨어진다. 특히 프렌치 프라이가 박스안에 같혀서 오래 있으면 고소하고 바삭거리는 식감을 내기 힘들다. 오늘은 큰맘먹고 매장에서 식사를 했다.입구는 넓어 보이지만 매장안에 테이블은 얼마 안된다. 입구 쪽을 확장해서 테이블을 몇개 더 두었다.에그롬과 로코모코, 그리고 하우스커피 한잔을 주문했다.에그룸에는 패티, 베이컨, 계란 후라이가 들어간다. 일반적인 버거 내용물에 계란이 들어 가서 그런지 부드러운 맛이 더해졌다. 하지만 이 구성에 단품 9,900원은 약간 부담스럽다.괌 여행을 갔을 때 처음 먹어본 로코모코. 기본 이상은 해준다. 남자 기준으로 양은 약간 부족한 편.양이 부족한 것 같아 치즈 프라이도 하나 주문했다. 포장해왔을 때는 눅눅해져서 다 먹지 못했는데 확실히 매장에서 먹는 것이 맛있다.요즘 수제버거 가게가 많이 생겨서 흔한 맛이지만, 고깃집과 술집이 대부분인 방이동 먹자골목에서는 흔치 않은 소중한 식당이다.총 평  음식맛 : ★★★☆☆  가성비 : ★★★☆☆  서비스 : ★★★☆☆  접근성 : ★★★★☆",
        "url": "/foody2"
    }
    ,
    
    "foody1": {
        "title": "(서울, 과천 맛집) 메밀장터",
            "author": "km.yu99",
            "category": "",
            "content": "지하철 4호선 선바위역에 위치한 막국수 맛집. 메밀장터  주 소 :  경기 과천시 뒷골로 5-7 선바위  영업시간 : 10:40 ~ 21:20(매일)  전화번호 : 02-504-0122  주요메뉴 : 들기름 막국수(9,000원), 명태회 막국수(9,000원)지하철역 인근에 있어서 도보로 이동가능하다.점심시간을 피해 오후 2시 이후에 방문하면 기다리지 않고 입장할 수 있다.아래 사진의 통로로 들어오면 가게 앞 주차장이 있다. 주차공간은 넓지 않은 편이다.사람들이 몰리는 시간대를 피해서 갔지만 주차할 공간이 없어서 인근에 주차해야 했다.식당 안에는 앉아서 식사할 수 있는 방과 테이블이 있다. 테이블은 만석이라 방으로 이동했다.이 곳의 대표 메뉴는 들기름 막국수이다. 개인적으로 비빔국수를 좋아해서 들기름 막국수, 명태회 막국수 하나씩 주문했다.비주얼은 나름 괜찮다. 면 색깔로 보아 메밀이 많이 포함된 것 같아 보이진 않았다.들기름 막국수. 계란 반쪽과 들깨 가루 고명, 들기름이 양념의 전부이다. 과연 이것으로 맛을 낼수 있을까?국수를 잘 비벼서 한젓갈 입에 넣어본다. 입안에 들기름의 고소함이 퍼진다. 지금까지 맛본 들기름 중에 단연 고소함이 으뜸이다.명태회 막국수도 괜찮다. 명태회가 충분이 들어있고 양념장도 간이 잘되어 있다.하지만 역시 이집의 으뜸은 들기름 막국수. 호불호가 갈릴수도 있지만, 두명이서 온다면 들기름, 명태회 막국수 하나씩 시켜서 쉐어하는 것도 괜찮아 보인다. 들기름 국수 면을 90% 정도 건저 먹은 후, 함께 나오는 동치미 국물을 조금 부어서 먹어보자. 이 또한 색다른 맛이다.물 막국수, 비빔 막국수에 익숙한 나에게 들기름 막국수의 세계로 안내한 가게였다.총 평  음식맛 : ★★★★☆  가성비 : ★★★☆☆  서비스 : ★★★☆☆  접근성 : ★★★★☆",
        "url": "/foody1"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://sguys99.github.io//">Happy Plant</a> &copy; 2021</section>
                <section class="poweredby">Published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    <a href="https://facebook.com/dbrhkdaud" target="_blank" rel="noopener">Facebook</a>
                    <!--
                    <a href="https://twitter.com/sguys99" target="_blank" rel="noopener">Twitter</a>
                    -->
                    <a href="https://www.linkedin.com/in/kmyu99" target="_blank" rel="noopener">LinkedIn</a>
                    <a href="https://github.com/sguys99" target="_blank" rel="noopener">Github</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-overlay-close" href="#"></a>
        <div class="subscribe-overlay-content">
            
            <h1 class="subscribe-overlay-title">Search Happy Plant</h1>
            <p class="subscribe-overlay-description">
                lunr.js를 이용한 posts 검색 </p>
            <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()"
               id="searchtext" type="text" name="searchtext"
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>

        </div>
    </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-69281367-1', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
